Advancing Biomedical Research Through Natural Language Processing and Large Language Models
A Case Study on Gene Expression Profiling and Knowledge Integration

	Antoni Czapski	Katarzyna Bloch
	Right Information	Right Information
	317214@uwr.edu.pl	katarzy.bloch@gmail.com
Abstract
The overwhelming volume of complex biomedical data, coupled with the sheer magnitude of scientific literature, makes it exceedingly difficult to process, analyze, and contextualize insights effectively. In this work, we present a pioneering approach to automatic information retrieval using publicly available gene expression profiling data from diverse sources, exemplifying a new paradigm in leveraging advanced natural language processing (NLP) and AI-based data analysis to transform biomedical research. Our methodology not only addresses the complex data analysis issues associated with high-dimensional �omic� datasets, but also provides a systematic and insightful way to integrate, interpret, and visualize results in the context of the most relevant scientific literature. We explore this approach in the context of a gene analysis study, showcasing the huge potential of AI-powered techniques to enhance the depth and precision of biomedical research.
   Key words: biomedical data analysis, natural language processing in genomics, large language models, AI-driven literature mining, gene expression profiling, precision medicine
1 Introduction
The acquisition of knowledge in the digital era frequently entails a distinct divergence in accessibility between ordinary and scholarly data. Although search engines such as Google are skilled at condensing our questions into easily digestible snippets that promptly provide the information we require, Bing takes it one step beyond by entirely removing the need to explore web pages, offering succinct, tailored answers in a conversational style. One can hardly imagine a more user-friendly experience, resembling a conversation with an expert in the field. Nonetheless, this intuitive searchability is markedly different from navigating scientific databases. Google Scholar, PubMed, and Web of Science seem rudimentary by comparison, lacking the capacity to interpret full-sentence queries, thus leaving users to sift through article titles with brief abstracts. To obtain the required insights, one must frequently navigate through a vast number of intricate and protracted works - an often time-consuming process that doesn�t support the overall goals of most researchers, who strive to ground their work within the broader context of scientific investigation.
   Consequently, we seek to address this common difficulty encountered by researchers who require a comprehensive perspective for their work. Our study presents a pioneering methodology that combines research findings with a comprehensive understanding of current scientific literature. We outline effective strategies for combining research outputs, automating the curation of relevant studies, and transforming this information into easily digestible formats utilizing cutting-edge Large Language Models. This approach creates a new pathway for researchers to interact with scholarly content in a more seamless and efficient manner.
1.1 Background
As we unravel the complexities of biological systems through scientific endeavors, the intricate web of biomedical data is rapidly expanding. Our pursuit to delve into this ever-growing web of knowledge presents significant challenges, as we strive to extract and integrate information seamlessly. Herein, we explore the current landscape of biomedical information mining, highlighting the ripe opportunities for enhancing the intrepid journey of data to discovery.
Current State of the Biomedical Information Mining
Biomedical information mining has remained a monolithic task for researchers, barely evolving in user experience over the last twenty years. Traditionally, researchers approach this Herculean task by conducting keyword searches across databases such as PubMed. They siphon, peruse, and integrate a selection of relevant manuscripts into their work, a labor-intensive ritual that has withstood the test of time. However, beneath the surface, search engine technology has undergone a renaissance, evolving from rudimentary phrase matching to sophisticated neural network-based recommender systems. Nevertheless, the upgrade in technology hasn�t quite permeated the methods by which scientific information is processed and delivered to users; there remains a gulf between advanced backend algorithms and the front-end user experience in scientific data inquiry.
Relevance of Natural Language Processing in Biomedical Research
Natural Language Processing (NLP) presents itself as a beacon for refinement in the realm of biomedical research. By simulating the human ability to understand language, NLP provides a powerful set of tools that can pierce through the dense fog of scientific jargon and literature, allowing for more astute aggregation and analysis of research data. It is through NLP that we can pioneer approaches to extract, interpret, and contextualize complex biological data, making them more accessible and informative for researchers.
Technological Advances Enabling NLP in Biomedicine
With advancements in computational power and algorithms, NLP has transitioned from a fledgling technology to a pivotal component in the biomedical informatics arsenal. Breakthroughs in machine learning, particularly in deep learning architectures such as transformers, have greatly enhanced the capabilities of NLP systems. These advancements have enabled NLP to elucidate and process the intricate patterns within large, complex datasets often found in biomedical research, fulfilling tasks from gene annotation to clinical decision support.
Advanced Data Analysis in Genomics
The field of genomics, characterized by its high-dimensional datasets, has advanced significantly, outpacing traditional analytical methodologies. The exponential growth of genomic data necessitates more sophisticated data analysis techniques capable of not just parsing but interpreting the vast and varied biological information. The granularity and depth of genomic studies now require analytical platforms imbued with NLP to mine the depths of relational intricacies among genetic expression profiles, phenotypic variations, and the associated literature.
   Embarking on this transformative voyage, we proceed to delineate the methodological advancements of information retrieval, processing, and integration amid the digital milieu. Our endeavor is directed toward reimagining the landscape in which information retrieval is not merely a static function but a dynamic portal to knowledge.
1. Information Retrieval - The quest begins amid the digital stacks of online repositories, where beyond retrieving scientific literature lies the opportunity for similarity searches that align scholarly documents with an investigator�s intellectual pursuits.
2. Information Processing - The cerebral craftsmanship of a researcher manifests in the ability to decipher, analyze, and synthesize extracted knowledge with the aim of fortifying their investigative repertoire.
3. Information Integration - Here, the empirical evidence garnered interlocks with the edifice of established knowledge, orchestrating a congruent symphony of past insights and novel discoveries.
   With this blueprint, we forge ahead to fortify the scaffold upon which data not only informs but transforms, ushering in a new era of artificial intelligence-assisted exploration within the biomedical sphere.
1.2 Motivation
The current process of background-check before conducting a study relies on retrieving the right articles and extracting domain-specific knowledge from them. The general principle is - the more knowledge the better. However, there is always a limit of time that could be spent on this phase of research. This often leads to underinformed decisions that may significantly impact the value of the study. The optimization of this process enabled by NLP techniques will move the focus of scientists from the manual labor of browsing online repositories with publications to more sophisticated tasks asking the data right questions and checking whether it can be answered by the current state of domain knowledge represented in collected (automatically) articles.
   In the case of our domain of research - analysis of cancer predisposition based on gene expression - as in many others the problem of �dialog� between what we do and what the �Science� says is undergoing through the whole research. Initially, we conducted a background check. As the research progressed, our focus shifted. We began to explore a new range of interests. This shift necessitated a thorough analysis of the literature about certain genes we deemed most important. Therefore the motivation behind NLP applications extends to merging the new knowledge (acquired during the research) with the current state of the scientific domain.
   All the present NLP approaches that we explored presented significant challenges in applications - from search engines discussed previously, to Large Language Models (like GPT family) trained on scientific knowledge. Regarding the second one, we analyze the potential of such methods based on the current state-of-the-art model - BioGPT [8]. The model is based on a relatively small architecture (7 Billion of parameters) that could be run on a small cluster of GPU. It can be utilized on a similar fashion to popular chatbots - users can ask questions and retrieve answers. The benefit of such a model is that out-of-the-box it presents biomedical expertise on the level of a graduate student. While it could be beneficial to discuss the problems with such a model, this approach poses a few unsolvable systematic problems regarding utilization that we described previously. Firstly, when a scientist asks a question crucial for his study, he seeks certainty in the answer, he wants to put the weight of the answer on the shoulders of previous studies, not on the opinion of someone - especially the AI system, that we do not fully understand. Secondly, such models are always outdated. The process of providing new knowledge to the model is achieved only during training, which is done once. To keep the model up-to-date it would be necessary to retrain it on each newly released publication.
2 Literature Review
Current State of Biomedical Information Retrieval
The contemporary landscape of information retrieval in biomedicine is predominantly concentrated on two pivotal objectives. The first is the development of superior, ready-to-use models�akin to specialized versions of ChatGPT tailored for biomedicine. This involves a competitive effort among researchers to design models capable of accurately answering biomedical queries, as typically encountered in university examinations. Notable instances of such models include MedCPT [5], BioGPT [8], and PubMedBERT [3]. While these chatbots may provide general medical information beneficial to laypersons seeking medical advice, they often fall short in meeting the more nuanced and current knowledge requirements of researchers, thereby limiting their utility in research contexts. The second objective focuses on refining the methods for identifying pertinent knowledge within scientific knowledge repositories. This area has evolved substantially, transitioning from basic word co-occurrence counting techniques to more sophisticated systems that emphasize the semantic comprehension of a paper�s subject matter. However, this evolution in the domain presents a dichotomy for researchers: systems either lack sufficient reliability or are not time-efficient. We propose a synergistic approach, amalgamating the strengths of both paradigms into a unified, end-to-end system that aims to optimize both accuracy and efficiency in biomedical information retrieval.
3 Research Objectives
This research is driven by two fundamental objectives - technical and visionary. The technical aspect encompasses the execution of a traditional study, adhering to established methodologies and analytical techniques. The visionary aspect, on the other hand, is dedicated to demonstrating and providing insight into the ways in which state-of-the-art technologies in Large Language Models (LLMs) and Natural Language Processing (NLP) can enhance and refine the quality of research. This dual approach not only solidifies the foundational aspects of our study but also propels it towards innovative frontiers in research methodology.
3.1 Technical
We conducted research to push further the current state of knowledge in the domain of cancer gene predisposition. We took under the lens a previous study [11] by Aaron M. Smith et al. The study was focused on finding the best predictive algorithm that based on gene expression tells whether a patient is healthy or has cancer. To find the algorithm that is truly best, the experiments were taken separately on 50 tasks regarding different cancer types. The need for improvement lies in the poor performance of previous algorithms, the problem was not solved successfully yet. It is the result of two main factors. Authors made an argument that genes rarely act in isolation, so it is reasonable to expect that combinations of genes may be more effective than individual genes for predicting phenotypes. This means that a too-simple predictive model might not catch the complexity of the patterns within the data. On the other hand for each of the 50 tasks we were dealing with about 55 thousand gene expression values per patient. The high amount of data mostly is preferable in ML problems, yet it was not the case here as the sample sizes were significantly smaller - from 100 to 1000 patients. This gives a problem of the �dimensionality curse� - each point became so distant from any other that finding patterns and similarities became almost impossible. The goal of our study was to replicate the results of the authors and study further the best algorithm in order to find the most discriminating genes for each tested cancer type.
3.2 Visionary
The visionary goal refers more to what was discussed before. We wanted to see how much further can be pushed the collaboration between data scientists and domain experts in biomedical research. To be more exact, we wanted to exemplify the pipeline of the following steps:
1. Problem identification - retrieving from the conducted research the areas that should be crosschecked with relevant literature
2. Automatic publication retrieval - how to gather thousands of texts that build the knowledgebase on given topic
3. Knowledge presentation - creating visualizations that present the intuitions and keypoints about the study results and scientific knowledge
4. Creation of expert system - creating an interactive system that can be asked about anything within the knowledge base and answers within natural language providing references for each given statement
   There are limitations of such techniques as ones discussed previously by Surendrabikram Thapa and Surabhi Adhikar in [12]. However, as we will showcase their concerns about lack of explainability and interoperability, limitations in domain-specifc knowledge and many other can be mitigated using clever NLP techniques (e.g. retrieval augmented generation).
4 Materials and Methods - Main Study
This study employs a comprehensive analysis using publicly accessible, extensive datasets: recount2 [2] and ARCHS4 [6]. These datasets encompass over 50,000 gene expression records from patients with various types of cancer. Our analysis utilizes the data preprocessed by Smith et al. in their research. We engaged 12 distinct sets of refined data, differentiated by the number of genes selected (labeled as �all�, �O�, �OT�) and the applied normalization methods, which include transcript per million (TPM), centered-log-ratio (CLR), Z-score, and Z-ternary. This approach ensures a robust and diverse analysis framework, catering to the complexity and scale of gene expression data in cancer research.
4.1 Methodology
To advance the analysis of this subject, it was imperative to first replicate the models. For this purpose, we trained all four recommended models across each of the 12 data sets, applying them to 50 distinct tasks. We explored various data embedding methods, including variational autoencoders, stochastic denoising autoencoders, principal component analysis, and scenarios without embedding. To ensure statistical robustness, each experiment was conducted using a 5-fold cross-validation approach. This was particularly crucial given the limited sample size of 100.
   The models evaluated in this study included logistic regression, random forest, k-nearest neighbors, and a novel architecture proposed by the original authors. This innovative approach is a semisupervised representation learning model, capitalizing on the network�s ability to construct representations of a typical healthy patient. This facilitates more effective classification of anomalies, such as cancer patients, by adapting the network to data specific to a particular type of cancer.
   Cumulatively, this comprehensive approach resulted in approximately 50,000 individual experiments, encompassing both training and testing phases. Each experiment was relatively rapid, typically concluding within a minute. While some processes were parallelized to enhance efficiency, the total computation time spanned around three weeks.
5 Results of Gene Expression Profiling
Hence, the initial analysis was focused on the identification and evaluation of a model which gives the best predictions. Once logistic regression was identified as the top predictive algorithm the next step

Figure 1: The figure presents a performance comparison of all trained models using data from the 5-fold cross-validation process. Each point represents a specific task, plotted on the x-axis, and the corresponding model�s accuracy on the y-axis. The marked points indicate the scores of the best models, which were determined by averaging the performance across all cross-validation trials. It was found that the best model, identified by the orange dot, is logistic regression. Interestingly, this conclusion aligns with the findings of the referenced parent article [11], where the best model�s accuracy is represented by the blue dot.
of the analysis was to identify the set of the most important genes.
   In our case, the top-1 predictive algorithm was logistic regression. It is a shared conclusion between our analysis and one performed in the study [11]. All of the results are shown in a Figure:1. The exact comparison is in the Table:1.
   Logistic regression is a simple linear model where the final output passes a sigmoid function. This characteristic enables it to differentiate between categorical rather than numerical data. In principle, the model as it examines the data tries to tune two parameters for each gene - "activation threshold" that separates sick from healthy and the certainty level of a such predictor. When the dataset goes through the process of normalization leading to obtaining the normal distribution for each feature, the so-called "certainty level" can be used as a determinant of feature importance.
   The outcome of the analysis is shown in Figure: 2. The visualization of gene importance for cancer prediction gives the opportunity to focus the following research on the ones that matter the most. It leads to new, data-driven questions. Why these particular genes are activated across most patients with thyroid cancer? Were there any other studies analyzing correlations between found genes and examined cancer types? To answer all those and the following questions we performed AI-based literature mining that gave key insights into scientific knowledge. For clarity, this paper will primarily discuss findings related to two specific cancer types: ccRCC and thyroid cancer. However, it should be noted that similar analyses were conducted offline for other cancer types explored in this research.
6 Results - Text Mining and Information Retrieval
Following the identification of key genes with significant discriminatory power, our next step was to delve deeper and examine the alignment of our findings with contemporary scientific literature. To accomplish this, we initially gathered a comprehensive collection of scientific publications related to the identified genes and the selected types of cancer. Our primary sources were the Pubmed Open Access Subset1 and articles from the OpenAlex [10] online repository. The selection process involved filtering publications based on the presence of specific key phrases and their synonyms, such as �thyroid
Research
Samples
Our best model (AUC)
Model from article (AUC)
COAD_stage
505
68.93%
69.97%
KIRC_stage
544
77.77%
78.79%
LIHC_stage
374
67.45%
67.18%
LUAD_stage
542
65.67%
63.78%
SKCM_stage
249
64.28%
61.17%
STAD_stage
416
65.65%
65.19%
THCA_stage
513
71.08%
70.16%
UCEC_stage
554
69.62%
68.19%
LUSC_stage
504
66.04%
66.27%
BRCA_stage
1134
61.95%
64.70%
CESC_grade
306
72.63%
72.01%
KIRC_grade
544
62.66%
62.94%
LGG_grade
532
77.96%
77.25%
LIHC_grade
374
68.25%
68.08%
PAAD_grade
179
66.83%
64.73%
STAD_grade
416
78.57%
77.83%
UCEC_grade
554
89.52%
89.56%
HNSC_grade
504
72.27%
73.56%
MEAN

70.40%
70.08%
Table 1: The table presents the accuracy scores of the best model from our study compared to the model from the reviewed previous study. It demonstrates that we were able to successfully replicate the findings of the previous study, which focused on identifying the best predictive algorithm. This provides a solid foundation for our further investigations into the interpretability of the model and the identification of the most important genes for prediction.
cancer�, �thyroid carcinoma� or �TPO�, �Thyroid peroxidase�.
   Subsequently, we employed various natural language processing techniques to extract pertinent information from these texts. Our approach was twofold: one that leveraged traditional text mining methodologies and another that utilized advanced large language models. We advocate this dualstrategy as an effective paradigm for conducting such analytical work in the realm of text mining and information retrieval.
6.1 Classical NLP approach
The Relation Map, shown in Figure: 3, is our way of merging the study results and the intuitions from large amounts of scientific publications. We had to point out the problem we faced. Having multiple thousands of pages of scientific articles regarding the topic we studied, we had to distill the intuition and present it in a digestible way. Our goal was to focus on the ratio of time spent on literature analysis vs the actual knowledge and intuitions gained related to the topic. The Relation Map is the aftermath of such a thought process. It is a joined visualization of key insights from our study shown in the context of scientific literature. Precisely it describes the relationship between two types of cancers and their most discriminating genes. Our analysis showed not only that exactly those eight genes are most influential for prediction but also what is their activation in a certain type of disease - color for indication and thickness for value. After we came to that conclusion, we focused on its scientific connotations.
   The first metric that we introduced was a Publication Correlation Matrix, which led to understanding which graph nodes should be connected to one another. In order to measure the correlation between two terms e.g. "ccRCC" and "TPO" we had to filter out only those publications that regard

Figure 2: After identifying the best predictive model, logistic regression, we applied it to distinguish between patients with clear cell renal cell carcinoma (ccRCC) and thyroid cancer. Using normalized gene expression data, we calculated the scaling factors to determine the importance of each gene. The y-axis represents the absolute value of the scaling factor, indicating the gene�s contribution to the classification. The x-axis displays the correlation score, measuring the relationship between gene expressions in patients with either ccRCC (0) or thyroid cancer (1).
such a connection. We were counting occurrences of "ccRCC" (and synonyms) in the "TPO" set of articles and respectively then the other way. Then, the logarithm of a joined number was our correlation factor between the two nodes. To highlight the gradient of low- to high-correlated terms we decided to express it as distances between two points.
6.1.1 Positioning graph nodes
We want to point out the challenge here. As we introduce a new approach it is necessary to create a custom solution. The calculated closeness of two points (both semantic and on our plot) does not lead directly to a final plot. The process of creating a graph requires knowing the positions of points, not the distances between them. In fact, mostly it is impossible to draw points on a 2D plain having their distances. For example distances 3cm, 1cm, 1cm between three points do not satisfy the triangle inequality, therefore a placement presenting the �closeness� intuition would put one of the points in superposition. We had to come up with an algorithm for a good-enough approximation of those measures.
   We propose an iterative algorithm, that starts with nodes at a given position (Figure: 4) and then slightly moves them to a desired state. As we saw during experiments process converges to a stable position. To validate the result of the algorithm we measured the relative error between the actual distances and desired ones. Mean error differs based on a given problem (i.e. set of genes/cancer types). However, the standard deviation is stable at a level of 5.4% across a hundred different starting positions. This observation convinces us that our approximation is reliable and in fact, visualizes the insights of the data even though the problem is unsolvable.
   Based on those observations we chose by hand an initial position of points that would be the best for visualization (Figure: 4). Then we presented the whole process of iterative data analysis as frames

Figure 3: Proximity Graph Visualizing Relationships Between Cancers and Their Most Discriminating Genes: A visual representation of abstract relations between concepts based on semantic proximity in scientific literature and clinical data. The graph highlights the connections between cancers (ccRCC, thyroid cancer) and their most discriminating genes, indicating gene regulation with color-coded markers (blue for down-regulated, red for up-regulated). The thickness of the lines reflects the magnitude of regulation change from the norm. The distances between nodes represent the semantic correlation, with closer points indicating extensive discussion in the literature. To maintain clarity, only major connections (19 out of 45) are displayed in the graph.
in an animation available on our website2.
6.1.2 Information Retrieval and ranking of publications
As the information retrieval process is the one that actually transcendence our research and goes beyond and within other research studies performed in a similar manner, we want go deeper. The following chapter shows how we retrieve knowledge and rank it from least to most relevant.
   Automatic information retrieval consists of two parts - data gathering and data presentation. To collect data, we sent queries to the online repository of the biomedical literature, which returned the list of articles ordered by their certain metric. To organize them according to our interests, i.e. how the article relates to the selected features, we had to define our own ranking.
   To produce a ranking, each publication had to be given a relevance score. To do this, we used two metrics. Firstly, we created a vector of occurrences of the �cancer phrase� in the following parts of the publication: title, abstract, results, conclusions and keywords. Secondly, we created a similar vector with the number of occurrences of the gene. The score was calculated using the following formula:
|V? |2+ |W? |2+V? � W? T
   Hence, articles with the highest relevance score where the ones with the highest number of search words co-occurances. This method takes into account scores of each phrase separately however, the

Figure 4: The Initial Positions for Graph Construction. The creation of the graph is a dynamic process which is visualized on our web page. To maintain clarity we chose this particular setting as process initialization, as it gives the most intuitlion while establishing the final position while not corrupting accuracy of the ideal position approximation.
most promoted articles are the ones with a large number of search-words co-occurrences.
   Analysis of 20000 publications generated a ranking of the most important publications for each cancer-gene and gene-gene relation. Publications with the best scores are hyperlinked to connections on Figure: 3. A full demonstration is available on our web page.
6.1.3 Publication context
Identifying the most relevant publication is not the end of the information retrieval process, but the good place to begin. Each scientific paper has directly correlated articles via citations. This enables researchers navigation through the knowledge graph created that way. However, the process of exploration conducted manually provides more problems than solutions:
1. Cited publications are not available freely or easily
2. There is no easy way of telling which of cited articles is worth reading
3. Publication can be cited by hundreds of others. How to get them and distinguish the ones relatedto our problem?
   In order to perform such an extended analysis we used the method from our previous research the Scientific Paper Advisor [13] tool, which addresses all these issues and solves them by providing an easy-to-use interface (Figure: 5).
   Presented graph work as an interactive map of currently analyzed publication (dot in the center) and others connected to it via citations. The horizontal axis indicates time passage. The ones on the left are the oldest, the ones on the right are referring to them. Each point represents the one article by displaying its connotations, title, authors, and scientific impact indicated by citations (shown as a dot size and directly in a hover). Moreover, each graph node is clickable and redirects to the publications� content. The user can expand the graph in any preferable direction receiving in that way information about the newest or more fundamental publications in a domain of interest.
   Each presented publication cites on average 20 others, which gives a massive, exponential growth of nodes in the mentioned graph of knowledge. This means that the width of the consecutive layers would be 1, 20, 400, 8000, and so on. The problem of finding publications referring to a central one goes into even bigger numbers. This is too much information to process by a human, therefore we had to automatically decide what is interesting and what is not.

Figure 5: Scientific Publication Citation Network - This directed graph represents the dynamic evolution of scientific knowledge. Each node represents a research article, while the connections indicate citation relationships. The graph provides context by visualizing articles that precede the center node (on the left) and those that build upon its findings (on the right). The size of each node reflects the importance of the publication, measured by the total number of citations. This network serves as a tool for exploring the contextual landscape surrounding a specific scientific publication.
   The most commonly used metric identifying the articles� impact is the citation count. It was used to distinguish what to show and what to hide. It turns out that it can be applied only to long-standing papers. For more recent ones this score gives unpredictable results. In order to overcome this problem, the neural network model was trained to predict citation count.
   The combination of the predictive model, automatic publication processing, and previous caserelated publication selection finally gave the basis for creating an interactive tool that the researcher can study.
6.2 Utilization of Large Language Models
The previous approach has shown that the NLP applications can be very case-specific within biomedical research. Manual publication research is time consuming. Here we present a way of utilizing NLP with an advanced LLM-based system to filter out the information of interest from the gathered publications. The best LLMs like GPT-3 [1], GPT-4 [9], Claude, and Mixtral are based on transformer neural network architecture. While these models provide the state-of-the-art capabilities, they can only process a small amount of text at once. This poses significant problems while we want to use such a system for retrieving and presenting knowledge from numerous scientific publications. To overcome this problem augmented generation system was used to retrieve the sufficient amount of text.
6.2.1 Retrieval-Augmented Generation
Retrieval-Augmented Generation (RAG) [7] is a method for integrating external knowledge into the generation process of large language models. It combines the power of pre-trained language models (like GPT-4), with the vast information available in external documents or databases. Here�s how it generally works:
1. Retrieval - When the model receives a prompt, it first retrieves relevant documents or data from an external source that could contain useful information for generating the response. This retrieval step can be based on various algorithms, including keyword search, semantic similarity, or more complex indexing methods.
2. Augmentation - The retrieved documents are then provided to the language model as additional context. The model uses this context to understand the topic better and generate a more informed and accurate response.
3. Generation - Finally, the model generates a response that is informed by both the original prompt and the information from the retrieved documents. This allows it to provide answers that are not solely based on the data it was trained on but also on up-to-date or detailed information from external sources.
   The RAG system is particularly useful for tasks where the language model needs to access factual information or specific knowledge that might not be contained within its training data. By using external documents, it can provide more accurate, detailed, and relevant responses.
   One of the key benefits of RAG is that it allows language models to stay up-to-date with the latest information without the need to be constantly retrained. It also enables the model to tap into a much larger pool of knowledge than what can be feasibly included in the training dataset.
   This approach has been used in various applications, including question-answering systems, where the ability to retrieve and use external information can significantly improve the quality of the responses.
6.2.2 Contextual chatbot
The contextual chatbot is a key tool utilized in this research to analyze the context of scientific publications. By leveraging advanced language models like GPT-4, Claude, or Gemini, the chatbot is able to understand and process user queries in the context of relevant publications. This approach allows for more accurate and informed responses by incorporating up-to-date information from external sources.
   The chatbot works by taking the user�s message and retrieving the necessary context from a selected source, such as a database of scientific literature. Instead of providing direct answers, the chatbot generates a prompt that combines the user�s query with the retrieved context information. The model then generates a response based on this combined input, resulting in more reliable and precise answers.
   By using a contextual chatbot, researchers can access and utilize a vast amount of scientific knowledge to inform their work. The chatbot acts as a bridge between researchers and the wealth of information available, providing accurate and relevant insights from the literature. This technology has the potential to greatly enhance the information retrieval process and support researchers in their quest for knowledge. Answers obtained in such a way are reliable and, based on our users� judgments, do not contain hallucinations. The final implementation is shown in Figure: 6 with demo available online3.
6.2.3 Information Retrieval - Matching Prompts to Contexts
The key component of the retrieval-augmented generation (RAG) system is the information retrieval process, specifically the matching of prompts to relevant contexts. This step is crucial for retrieving the most pertinent information from the database and incorporating it into the generation process.
   In this research, the RAG system utilizes a specialized vector database that stores small chunks of text, typically paragraphs, from the knowledge base. Each text is transformed into a numerical representation known as an embedding, which captures its meaning in a numerical format.
   When a prompt is given to the RAG system, it is also transformed into an embedding. The search algorithm then compares the prompt�s embedding to the embeddings of the contexts in the database, using a metric such as cosine similarity to measure the similarity between the prompt and each context.

Figure 6: Part of the conversation about publication [4] regarding the correlation of the TSHR gene and thyroid cancer. Conversation taken between bio-medicine expert and semantic question-answering system based on RAG method.
The algorithm retrieves the contexts with the closest embeddings to the prompt, indicating the most relevant information for generating a response.
   This matching process allows the RAG system to retrieve the most relevant contexts based on the given prompt, ensuring that the generated response incorporates up-to-date and accurate information from the external knowledge base.
6.2.4 Expanding RAG System with Unlimited Context
The RAG system addresses the limitation of limited source knowledge by extending its capabilities to larger context sizes. While publicly available models from OpenAI can currently handle up to 32k tokens (equivalent to approximately 50 pages of text), our technique connects the model to a database consisting of over 1,000 such 50-page-long publications.
   With this expanded context, the RAG system can retrieve and incorporate a significantly larger amount of information for generating responses. The results of querying this robust system are illustrated in Figure 7. The RAG model was tasked with retrieving all relevant information about the correlations of the listed entities, such as the impact of ccRCC on proteins and the influence of genes on thyroid cancer.

Figure 7: Summarized Answers Table - This table presents the responses generated by our chatbot for various correlations between column and row terms, such as the relationship between proteins and thyroid cancer. Notably, these answers are derived from a significantly larger context, encompassing hundreds of relevant publications.
   Of particular interest is the pair of lipids and thyroid cancer, where the model showcases an uncommon behavior. Instead of providing speculative answers, the model openly admits that it does not possess knowledge on this specific relationship. This level of transparency and caution is not commonly observed in language models, highlighting the RAG system�s ability to provide more reliable and grounded responses.
   By leveraging an extensive database and expanding the context available to the RAG system, researchers can access a significantly larger wealth of knowledge. This advancement allows for more accurate, detailed, and nuanced answers to a wide range of queries, enhancing the information retrieval process in biomedicine.
6.3 Potential Applications
The research methodology and tools presented in this study have a wide range of potential applications in the field of biomedicine. Some specific scenarios where this research methodology could be applied include:
1. Gene Expression Analysis - The methodology demonstrated in this study can be applied to analyze gene expression data in different disease contexts. By identifying key genes and their relationships to specific diseases, researchers can gain a deeper understanding of the underlying mechanisms and potential treatment options.
2. Precision Medicine - The integration of advanced NLP techniques and large language models can greatly enhance the field of precision medicine. By leveraging vast amounts of scientific literature, researchers can retrieve and synthesize knowledge to inform personalized treatment plans based on individual patient characteristics.
3. Drug Discovery - The information retrieval and analysis techniques showcased in this study can be utilized to expedite the process of drug discovery. By mining scientific literature, researchers can identify potential drug targets, understand the mechanisms of action, and optimize drug design.
4. Clinical Decision Support - The combination of data analysis and information retrieval can be leveraged to develop clinical decision support systems. These systems can assist healthcare professionals in making informed decisions based on current scientific knowledge and patientspecific data.
5. Biomarker Identification - The research methodology presented in this study can be applied to identify biomarkers for various diseases. By analyzing gene expression data and mining relevant literature, researchers can uncover potential biomarkers that can aid in early diagnosis, risk assessment, and monitoring of disease progression.
6. Data Integration - By utilizing NLP techniques, researchers can integrate data from various sources, including electronic health records, genomic databases, and scientific literature. This integration of diverse datasets allows for a more holistic understanding of disease mechanisms and treatment options.
   Anticipated technological and methodological advancements, such as the continuous improvement of large language models and advancements in data integration techniques, will further enhance the future of NLP in biomedicine. These advancements will enable more accurate and efficient information retrieval, more sophisticated data analysis, and deeper insights into complex biological systems.
7 Conclusion and Future Work
7.1 Summary
In this research, we explored the integration of advanced Natural Language Processing (NLP) techniques and large language models to enhance biomedical research. Our study focused on the analysis of gene expression data in the context of cancer predisposition. We replicated and extended a previous study�s findings, identifying the most discriminative genes for cancer prediction. To enrich our analysis and connect it with the scientific literature, we employed text-mining strategies to gather relevant publications and extract key insights. We utilized a combination of traditional NLP approaches and the capabilities of large language models to optimize the information retrieval process and provide accurate and up-to-date knowledge. Our research demonstrates the potential of NLP in improving data analysis, knowledge integration, and decision support in the field of biomedicine. The presented methodology opens opportunities for precise and personalized medicine, drug discovery, biomarker identification, and more. Anticipated future advancements in NLP and data integration will further enhance the capabilities and impact of this research.
7.2 Future Directions
Moving forward, there are several areas of future exploration and improvement for the research conducted in this study. These areas include, but are not limited to:
1. Enhancing Information Retrieval - While the methodology presented in this research has demonstrated effective information retrieval and knowledge synthesis, there is still room for improvement. Future work can focus on refining the retrieval algorithms to improve the accuracy and relevance of the retrieved information. This can be achieved through the development of more sophisticated matching techniques such as reranking, and diversifying retrieved paragraphs.
2. Expanding the Knowledge Base - The current research utilized a substantial knowledge base of scientific publications. However, the knowledge base can be further expanded to include additional sources of information, such as clinical trial databases, electronic health records, and genomic datasets. By incorporating a broader range of data sources, researchers can gain access to a more comprehensive and diverse set of knowledge to inform their research.
3. Improving Contextual Chatbot Capabilities - While the contextual chatbot used in this research has proven to be a valuable tool for analyzing scientific publications, there is still potential for improvement. Future work can focus on further developing the chatbot�s capabilities, such as improving its understanding of complex queries, generating more detailed responses, and addressing limitations in hallucination or incorrect information.
4. Advancements in Large Language Models - The field of large language models is rapidly evolving, with new models and architectures being developed regularly. Future work can explore the application of more advanced models, such as GPT-4, Claude, Gemini, or Mixtral to further enhance the capabilities and performance of the research system. This includes leveraging newer models for both information retrieval and generation tasks, as well as incorporating improvements in areas such as explainability, robustness, and handling of domain-specific knowledge.
5. Interdisciplinary Collaboration - The future of NLP in biomedicine lies in strong collaboration between data scientists and biomedical researchers. Future work should focus on fostering interdisciplinary collaboration and knowledge exchange, allowing for the co-development of tools and methodologies that address domain-specific challenges and leverage the expertise of all stakeholders.
6. Ethical Considerations - As with any emerging technology, ethical considerations play a crucial role in the future development and deployment of NLP in biomedicine. Future work should address issues such as data privacy, bias in training data, and transparency of model decisionmaking. It is important to ensure that the benefits of NLP are balanced with ethical considerations to maximize the potential positive impact on biomedical research.
   Anticipated technological and methodological advancements in NLP, such as the further development of large language models, advancements in data integration techniques, and improvements in explainability and interpretability, will continue to shape the future of NLP in biomedicine. By addressing these future directions, researchers can unlock the full potential of NLP in advancing biomedical knowledge and improving patient care.