{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"[API-KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 25feb695-506e-4fc6-a66e-b89b8ae6dfa5\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "document = LlamaParse(result_type=\"markdown\").load_data(file_path=\"[location].txt")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='113b8c24-9b4f-4998-8c29-4e8d64e0de21', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Advancing Biomedical Research Through Natural Language Processing and Large Language Models\\n\\n# A Case Study on Gene Expression Profiling and Knowledge Integration\\n\\nAntoni Czapski\\n\\nRight Information\\n\\n317214@uwr.edu.pl\\n\\nKatarzyna Bloch\\n\\nRight Information\\n\\nkatarzy.bloch@gmail.com\\n\\n# Abstract\\n\\nThe overwhelming volume of complex biomedical data, coupled with the sheer magnitude of scientific literature, makes it exceedingly difficult to process, analyze, and contextualize insights effectively. In this work, we present a pioneering approach to automatic information retrieval using publicly available gene expression profiling data from diverse sources, exemplifying a new paradigm in leveraging advanced natural language processing (NLP) and AI-based data analysis to transform biomedical research. Our methodology not only addresses the complex data analysis issues associated with high-dimensional ’omic’ datasets, but also provides a systematic and insightful way to integrate, interpret, and visualize results in the context of the most relevant scientific literature. We explore this approach in the context of a gene analysis study, showcasing the huge potential of AI-powered techniques to enhance the depth and precision of biomedical research.\\n\\nKey words: biomedical data analysis, natural language processing in genomics, large language models, AI-driven literature mining, gene expression profiling, precision medicine\\n\\n# 1 Introduction\\n\\nThe acquisition of knowledge in the digital era frequently entails a distinct divergence in accessibility between ordinary and scholarly data. Although search engines such as Google are skilled at condensing our questions into easily digestible snippets that promptly provide the information we require, Bing takes it one step beyond by entirely removing the need to explore web pages, offering succinct, tailored answers in a conversational style. One can hardly imagine a more user-friendly experience, resembling a conversation with an expert in the field. Nonetheless, this intuitive searchability is markedly different from navigating scientific databases. Google Scholar, PubMed, and Web of Science seem rudimentary by comparison, lacking the capacity to interpret full-sentence queries, thus leaving users to sift through article titles with brief abstracts. To obtain the required insights, one must frequently navigate through a vast number of intricate and protracted works - an often time-consuming process that doesn’t support the overall goals of most researchers, who strive to ground their work within the broader context of scientific investigation.\\n\\nConsequently, we seek to address this common difficulty encountered by researchers who require a comprehensive perspective for their work. Our study presents a pioneering methodology that combines research findings with a comprehensive understanding of current scientific literature. We outline effective strategies for combining research outputs, automating the curation of relevant studies, and', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2ea2cd5d-4583-4352-ad8a-e3c7bc188343', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='transforming this information into easily digestible formats utilizing cutting-edge Large Language Models. This approach creates a new pathway for researchers to interact with scholarly content in a more seamless and efficient manner.\\n\\n# 1.1 Background\\n\\nAs we unravel the complexities of biological systems through scientific endeavors, the intricate web of biomedical data is rapidly expanding. Our pursuit to delve into this ever-growing web of knowledge presents significant challenges, as we strive to extract and integrate information seamlessly. Herein, we explore the current landscape of biomedical information mining, highlighting the ripe opportunities for enhancing the intrepid journey of data to discovery.\\n\\n# Current State of the Biomedical Information Mining\\n\\nBiomedical information mining has remained a monolithic task for researchers, barely evolving in user experience over the last twenty years. Traditionally, researchers approach this Herculean task by conducting keyword searches across databases such as PubMed. They siphon, peruse, and integrate a selection of relevant manuscripts into their work, a labor-intensive ritual that has withstood the test of time. However, beneath the surface, search engine technology has undergone a renaissance, evolving from rudimentary phrase matching to sophisticated neural network-based recommender systems. Nevertheless, the upgrade in technology hasn’t quite permeated the methods by which scientific information is processed and delivered to users; there remains a gulf between advanced backend algorithms and the front-end user experience in scientific data inquiry.\\n\\n# Relevance of Natural Language Processing in Biomedical Research\\n\\nNatural Language Processing (NLP) presents itself as a beacon for refinement in the realm of biomedical research. By simulating the human ability to understand language, NLP provides a powerful set of tools that can pierce through the dense fog of scientific jargon and literature, allowing for more astute aggregation and analysis of research data. It is through NLP that we can pioneer approaches to extract, interpret, and contextualize complex biological data, making them more accessible and informative for researchers.\\n\\n# Technological Advances Enabling NLP in Biomedicine\\n\\nWith advancements in computational power and algorithms, NLP has transitioned from a fledgling technology to a pivotal component in the biomedical informatics arsenal. Breakthroughs in machine learning, particularly in deep learning architectures such as transformers, have greatly enhanced the capabilities of NLP systems. These advancements have enabled NLP to elucidate and process the intricate patterns within large, complex datasets often found in biomedical research, fulfilling tasks from gene annotation to clinical decision support.\\n\\n# Advanced Data Analysis in Genomics\\n\\nThe field of genomics, characterized by its high-dimensional datasets, has advanced significantly, outpacing traditional analytical methodologies. The exponential growth of genomic data necessitates more sophisticated data analysis techniques capable of not just parsing but interpreting the vast and varied biological information. The granularity and depth of genomic studies now require analytical platforms imbued with NLP to mine the depths of relational intricacies among genetic expression profiles, phenotypic variations, and the associated literature.\\n\\nEmbarking on this transformative voyage, we proceed to delineate the methodological advancements of information retrieval, processing, and integration amid the digital milieu. Our endeavor is directed toward reimagining the landscape in which information retrieval is not merely a static function but a dynamic portal to knowledge.\\n\\n# 1. Information Retrieval\\n\\nThe quest begins amid the digital stacks of online repositories, where', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7dd72699-6c70-41b9-b8b1-8f7bb738d2dc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 1.1 Information Processing\\n\\nbeyond retrieving scientific literature lies the opportunity for similarity searches that align scholarly documents with an investigator’s intellectual pursuits.\\n\\n# 2. Information Processing\\n\\nThe cerebral craftsmanship of a researcher manifests in the ability to decipher, analyze, and synthesize extracted knowledge with the aim of fortifying their investigative repertoire.\\n\\n# 3. Information Integration\\n\\nHere, the empirical evidence garnered interlocks with the edifice of established knowledge, orchestrating a congruent symphony of past insights and novel discoveries.\\n\\nWith this blueprint, we forge ahead to fortify the scaffold upon which data not only informs but transforms, ushering in a new era of artificial intelligence-assisted exploration within the biomedical sphere.\\n\\n# 1.2 Motivation\\n\\nThe current process of background-check before conducting a study relies on retrieving the right articles and extracting domain-specific knowledge from them. The general principle is - the more knowledge the better. However, there is always a limit of time that could be spent on this phase of research. This often leads to underinformed decisions that may significantly impact the value of the study. The optimization of this process enabled by NLP techniques will move the focus of scientists from the manual labor of browsing online repositories with publications to more sophisticated tasks - asking the data right questions and checking whether it can be answered by the current state of domain knowledge represented in collected (automatically) articles.\\n\\nIn the case of our domain of research - analysis of cancer predisposition based on gene expression - as in many others the problem of ’dialog’ between what we do and what the ’Science’ says is undergoing through the whole research. Initially, we conducted a background check. As the research progressed, our focus shifted. We began to explore a new range of interests. This shift necessitated a thorough analysis of the literature about certain genes we deemed most important. Therefore the motivation behind NLP applications extends to merging the new knowledge (acquired during the research) with the current state of the scientific domain.\\n\\nAll the present NLP approaches that we explored presented significant challenges in applications - from search engines discussed previously, to Large Language Models (like GPT family) trained on scientific knowledge. Regarding the second one, we analyze the potential of such methods based on the current state-of-the-art model - BioGPT [8]. The model is based on a relatively small architecture (7 Billion of parameters) that could be run on a small cluster of GPU. It can be utilized on a similar fashion to popular chatbots - users can ask questions and retrieve answers. The benefit of such a model is that out-of-the-box it presents biomedical expertise on the level of a graduate student. While it could be beneficial to discuss the problems with such a model, this approach poses a few unsolvable systematic problems regarding utilization that we described previously. Firstly, when a scientist asks a question crucial for his study, he seeks certainty in the answer, he wants to put the weight of the answer on the shoulders of previous studies, not on the opinion of someone - especially the AI system, that we do not fully understand. Secondly, such models are always outdated. The process of providing new knowledge to the model is achieved only during training, which is done once. To keep the model up-to-date it would be necessary to retrain it on each newly released publication.\\n\\n# 2 Literature Review\\n\\n# Current State of Biomedical Information Retrieval', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6d155c6e-bbf9-4461-92a9-509f485eaa81', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The contemporary landscape of information retrieval in biomedicine is predominantly concentrated on two pivotal objectives. The first is the development of superior, ready-to-use models—akin to specialized versions of ChatGPT tailored for biomedicine. This involves a competitive effort among researchers to design models capable of accurately answering biomedical queries, as typically encountered in university examinations. Notable instances of such models include MedCPT [5], BioGPT [8], and PubMedBERT [3]. While these chatbots may provide general medical information beneficial to laypersons seeking medical advice, they often fall short in meeting the more nuanced and current knowledge requirements of researchers, thereby limiting their utility in research contexts. The second objective focuses on refining the methods for identifying pertinent knowledge within scientific knowledge repositories. This area has evolved substantially, transitioning from basic word co-occurrence counting techniques to more sophisticated systems that emphasize the semantic comprehension of a paper’s subject matter. However, this evolution in the domain presents a dichotomy for researchers: systems either lack sufficient reliability or are not time-efficient. We propose a synergistic approach, amalgamating the strengths of both paradigms into a unified, end-to-end system that aims to optimize both accuracy and efficiency in biomedical information retrieval.\\n\\n# 3 Research Objectives\\n\\nThis research is driven by two fundamental objectives - technical and visionary. The technical aspect encompasses the execution of a traditional study, adhering to established methodologies and analytical techniques. The visionary aspect, on the other hand, is dedicated to demonstrating and providing insight into the ways in which state-of-the-art technologies in Large Language Models (LLMs) and Natural Language Processing (NLP) can enhance and refine the quality of research. This dual approach not only solidifies the foundational aspects of our study but also propels it towards innovative frontiers in research methodology.\\n\\n# 3.1 Technical\\n\\nWe conducted research to push further the current state of knowledge in the domain of cancer gene predisposition. We took under the lens a previous study [11] by Aaron M. Smith et al. The study was focused on finding the best predictive algorithm that based on gene expression tells whether a patient is healthy or has cancer. To find the algorithm that is truly best, the experiments were taken separately on 50 tasks regarding different cancer types. The need for improvement lies in the poor performance of previous algorithms, the problem was not solved successfully yet. It is the result of two main factors. Authors made an argument that genes rarely act in isolation, so it is reasonable to expect that combinations of genes may be more effective than individual genes for predicting phenotypes. This means that a too-simple predictive model might not catch the complexity of the patterns within the data. On the other hand for each of the 50 tasks we were dealing with about 55 thousand gene expression values per patient. The high amount of data mostly is preferable in ML problems, yet it was not the case here as the sample sizes were significantly smaller - from 100 to 1000 patients. This gives a problem of the ’dimensionality curse’ - each point became so distant from any other that finding patterns and similarities became almost impossible. The goal of our study was to replicate the results of the authors and study further the best algorithm in order to find the most discriminating genes for each tested cancer type.\\n\\n# 3.2 Visionary\\n\\nThe visionary goal refers more to what was discussed before. We wanted to see how much further can be pushed the collaboration between data scientists and domain experts in biomedical research. To be more exact, we wanted to exemplify the pipeline of the following steps:', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9f7389e6-f17b-44d2-abda-dc5998bb1d23', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 1. Problem identification\\n\\nretrieving from the conducted research the areas that should be cross-checked with relevant literature\\n\\n# 2. Automatic publication retrieval\\n\\nhow to gather thousands of texts that build the knowledge-base on given topic\\n\\n# 3. Knowledge presentation\\n\\ncreating visualizations that present the intuitions and keypoints about the study results and scientific knowledge\\n\\n# 4. Creation of expert system\\n\\ncreating an interactive system that can be asked about anything within the knowledge base and answers within natural language providing references for each given statement\\n\\nThere are limitations of such techniques as ones discussed previously by Surendrabikram Thapa and Surabhi Adhikar in [12]. However, as we will showcase their concerns about lack of explainability and interoperability, limitations in domain-specific knowledge and many other can be mitigated using clever NLP techniques (e.g. retrieval augmented generation).\\n\\n# 4. Materials and Methods - Main Study\\n\\nThis study employs a comprehensive analysis using publicly accessible, extensive datasets: recount2 [2] and ARCHS4 [6]. These datasets encompass over 50,000 gene expression records from patients with various types of cancer. Our analysis utilizes the data preprocessed by Smith et al. in their research. We engaged 12 distinct sets of refined data, differentiated by the number of genes selected (labeled as ’all’, ’O’, ’OT’) and the applied normalization methods, which include transcript per million (TPM), centered-log-ratio (CLR), Z-score, and Z-ternary. This approach ensures a robust and diverse analysis framework, catering to the complexity and scale of gene expression data in cancer research.\\n\\n# 4.1 Methodology\\n\\nTo advance the analysis of this subject, it was imperative to first replicate the models. For this purpose, we trained all four recommended models across each of the 12 data sets, applying them to 50 distinct tasks. We explored various data embedding methods, including variational autoencoders, stochastic denoising autoencoders, principal component analysis, and scenarios without embedding. To ensure statistical robustness, each experiment was conducted using a 5-fold cross-validation approach. This was particularly crucial given the limited sample size of 100.\\n\\nThe models evaluated in this study included logistic regression, random forest, k-nearest neighbors, and a novel architecture proposed by the original authors. This innovative approach is a semi-supervised representation learning model, capitalizing on the network’s ability to construct representations of a typical healthy patient. This facilitates more effective classification of anomalies, such as cancer patients, by adapting the network to data specific to a particular type of cancer.\\n\\nCumulatively, this comprehensive approach resulted in approximately 50,000 individual experiments, encompassing both training and testing phases. Each experiment was relatively rapid, typically concluding within a minute. While some processes were parallelized to enhance efficiency, the total computation time spanned around three weeks.\\n\\n# 5 Results of Gene Expression Profiling\\n\\nHence, the initial analysis was focused on the identification and evaluation of a model which gives the best predictions. Once logistic regression was identified as the top predictive algorithm the next step', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bb87830e-4094-4f09-adc7-39b81eaafab4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Figure 1\\n\\nThe figure presents a performance comparison of all trained models using data from the 5-fold cross-validation process. Each point represents a specific task, plotted on the x-axis, and the corresponding model’s accuracy on the y-axis. The marked points indicate the scores of the best models, which were determined by averaging the performance across all cross-validation trials. It was found that the best model, identified by the orange dot, is logistic regression. Interestingly, this conclusion aligns with the findings of the referenced parent article [11], where the best model’s accuracy is represented by the blue dot.\\n\\nThe analysis was to identify the set of the most important genes. In our case, the top-1 predictive algorithm was logistic regression. It is a shared conclusion between our analysis and one performed in the study [11]. All of the results are shown in Figure 1. The exact comparison is in Table 1.\\n\\nLogistic regression is a simple linear model where the final output passes a sigmoid function. This characteristic enables it to differentiate between categorical rather than numerical data. In principle, the model as it examines the data tries to tune two parameters for each gene - \"activation threshold\" that separates sick from healthy and the certainty level of such a predictor. When the dataset goes through the process of normalization leading to obtaining the normal distribution for each feature, the so-called \"certainty level\" can be used as a determinant of feature importance.\\n\\n# Figure 2\\n\\nThe outcome of the analysis is shown in Figure 2. The visualization of gene importance for cancer prediction gives the opportunity to focus the following research on the ones that matter the most. It leads to new, data-driven questions. Why these particular genes are activated across most patients with thyroid cancer? Were there any other studies analyzing correlations between found genes and examined cancer types? To answer all those and the following questions we performed AI-based literature mining that gave key insights into scientific knowledge. For clarity, this paper will primarily discuss findings related to two specific cancer types: ccRCC and thyroid cancer. However, it should be noted that similar analyses were conducted offline for other cancer types explored in this research.\\n\\n# 6 Results - Text Mining and Information Retrieval\\n\\nFollowing the identification of key genes with significant discriminatory power, our next step was to delve deeper and examine the alignment of our findings with contemporary scientific literature. To accomplish this, we initially gathered a comprehensive collection of scientific publications related to the identified genes and the selected types of cancer. Our primary sources were the Pubmed Open Access Subset1 and articles from the OpenAlex [10] online repository. The selection process involved filtering publications based on the presence of specific key phrases and their synonyms, such as ’thyroid.\\n\\n1 www.ncbi.nlm.nih.gov/pmc/tools/openftlist/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9a8a33b-95b9-4554-86bf-327f42fbddeb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Research\\n\\n| |Samples|Our best model (AUC)|Model from article (AUC)|\\n|---|---|---|---|\\n|COAD_stage|505|68.93%|69.97%|\\n|KIRC_stage|544|77.77%|78.79%|\\n|LIHC_stage|374|67.45%|67.18%|\\n|LUAD_stage|542|65.67%|63.78%|\\n|SKCM_stage|249|64.28%|61.17%|\\n|STAD_stage|416|65.65%|65.19%|\\n|THCA_stage|513|71.08%|70.16%|\\n|UCEC_stage|554|69.62%|68.19%|\\n|LUSC_stage|504|66.04%|66.27%|\\n|BRCA_stage|1134|61.95%|64.70%|\\n|CESC_grade|306|72.63%|72.01%|\\n|KIRC_grade|544|62.66%|62.94%|\\n|LGG_grade|532|77.96%|77.25%|\\n|LIHC_grade|374|68.25%|68.08%|\\n|PAAD_grade|179|66.83%|64.73%|\\n|STAD_grade|416|78.57%|77.83%|\\n|UCEC_grade|554|89.52%|89.56%|\\n|HNSC_grade|504|72.27%|73.56%|\\n|MEAN|MEAN|70.40%|70.08%| |\\n\\nTable 1: The table presents the accuracy scores of the best model from our study compared to the model from the reviewed previous study. It demonstrates that we were able to successfully replicate the findings of the previous study, which focused on identifying the best predictive algorithm. This provides a solid foundation for our further investigations into the interpretability of the model and the identification of the most important genes for prediction.\\n\\ncancer’, ’thyroid carcinoma’ or ’TPO’, ’Thyroid peroxidase’. Subsequently, we employed various natural language processing techniques to extract pertinent information from these texts. Our approach was twofold: one that leveraged traditional text mining methodologies and another that utilized advanced large language models. We advocate this dual-strategy as an effective paradigm for conducting such analytical work in the realm of text mining and information retrieval.\\n\\n# 6.1 Classical NLP approach\\n\\nThe Relation Map, shown in Figure: 3, is our way of merging the study results and the intuitions from large amounts of scientific publications. We had to point out the problem we faced. Having multiple thousands of pages of scientific articles regarding the topic we studied, we had to distill the intuition and present it in a digestible way. Our goal was to focus on the ratio of time spent on literature analysis vs the actual knowledge and intuitions gained related to the topic. The Relation Map is the aftermath of such a thought process. It is a joined visualization of key insights from our study shown in the context of scientific literature. Precisely it describes the relationship between two types of cancers and their most discriminating genes. Our analysis showed not only that exactly those eight genes are most influential for prediction but also what is their activation in a certain type of disease - color for indication and thickness for value. After we came to that conclusion, we focused on its scientific connotations.\\n\\nThe first metric that we introduced was a Publication Correlation Matrix, which led to understanding which graph nodes should be connected to one another. In order to measure the correlation between two terms e.g. \"ccRCC\" and \"TPO\" we had to filter out only those publications that regard', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cdc1bbb2-3bd6-4651-99fc-19b78903365b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# HOX7 HOXB-As3\\n\\n# FXYD2 TPO\\n\\n# HOX2\\n\\n# TSHR\\n\\n# HOXB7\\n\\n# IYD\\n\\n# LINCO2036\\n\\n# 1\\n\\n# 1L\\n\\n# Thyroid cancer\\n\\n# ccRCC\\n\\n# Predictiveness of cancer type\\n\\nFigure 2: After identifying the best predictive model, logistic regression, we applied it to distinguish between patients with clear cell renal cell carcinoma (ccRCC) and thyroid cancer. Using normalized gene expression data, we calculated the scaling factors to determine the importance of each gene. The y-axis represents the absolute value of the scaling factor, indicating the gene’s contribution to the classification. The x-axis displays the correlation score, measuring the relationship between gene expressions in patients with either ccRCC (0) or thyroid cancer (1).\\n\\nsuch a connection. We were counting occurrences of \"ccRCC\" (and synonyms) in the \"TPO\" set of articles and respectively then the other way. Then, the logarithm of a joined number was our correlation factor between the two nodes. To highlight the gradient of low- to high-correlated terms we decided to express it as distances between two points.\\n\\n# 6.1.1 Positioning graph nodes\\n\\nWe want to point out the challenge here. As we introduce a new approach it is necessary to create a custom solution. The calculated closeness of two points (both semantic and on our plot) does not lead directly to a final plot. The process of creating a graph requires knowing the positions of points, not the distances between them. In fact, mostly it is impossible to draw points on a 2D plain having their distances. For example distances 3cm, 1cm, 1cm between three points do not satisfy the triangle inequality, therefore a placement presenting the ’closeness’ intuition would put one of the points in superposition. We had to come up with an algorithm for a good-enough approximation of those measures.\\n\\nWe propose an iterative algorithm, that starts with nodes at a given position (Figure: 4) and then slightly moves them to a desired state. As we saw during experiments process converges to a stable position. To validate the result of the algorithm we measured the relative error between the actual distances and desired ones. Mean error differs based on a given problem (i.e. set of genes/cancer types). However, the standard deviation is stable at a level of 5.4% across a hundred different starting positions. This observation convinces us that our approximation is reliable and in fact, visualizes the insights of the data even though the problem is unsolvable.\\n\\nBased on those observations we chose by hand an initial position of points that would be the best for visualization (Figure: 4). Then we presented the whole process of iterative data analysis as frames\\n\\n8', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ddeef5fd-44d8-49b1-9d1e-9704e731fce0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"# Figure 3: Proximity Graph Visualizing Relationships Between Cancers and Their Most Discriminating Genes\\n\\nA visual representation of abstract relations between concepts based on semantic proximity in scientific literature and clinical data. The graph highlights the connections between cancers (ccRCC, thyroid cancer) and their most discriminating genes, indicating gene regulation with color-coded markers (blue for down-regulated, red for up-regulated). The thickness of the lines reflects the magnitude of regulation change from the norm. The distances between nodes represent the semantic correlation, with closer points indicating extensive discussion in the literature. To maintain clarity, only major connections (19 out of 45) are displayed in the graph.\\n\\nin an animation available on our website2.\\n\\n# 6.1.2 Information Retrieval and ranking of publications\\n\\nAs the information retrieval process is the one that actually transcendence our research and goes beyond and within other research studies performed in a similar manner, we want go deeper. The following chapter shows how we retrieve knowledge and rank it from least to most relevant.\\n\\nAutomatic information retrieval consists of two parts - data gathering and data presentation. To collect data, we sent queries to the online repository of the biomedical literature, which returned the list of articles ordered by their certain metric. To organize them according to our interests, i.e. how the article relates to the selected features, we had to define our own ranking.\\n\\nTo produce a ranking, each publication had to be given a relevance score. To do this, we used two metrics. Firstly, we created a vector of occurrences of the 'cancer phrase' in the following parts of the publication: title, abstract, results, conclusions and keywords. Secondly, we created a similar vector with the number of occurrences of the gene. The score was calculated using the following formula:\\n\\n|⃗V |2 + |W|2 + V × WT\\n\\nHence, articles with the highest relevance score where the ones with the highest number of search words co-occurrences. This method takes into account scores of each phrase separately however, the\\n\\nhttps://biomedical.dev6.rightinformation.com/genomic-analysis\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='01758826-7f63-403a-9075-8940c5eac847', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# HOXB7\\n\\n# Legend:\\n\\n|HOXB2|Cancer Type|\\n|---|---|\\n|HOXB-AS3|Gene|\\n|Thyroid|IYD|\\n|ccRCC| |\\n|Hox2| |\\n|SFTA3| |\\n|TSHR| |\\n|TPO| |\\n\\nFigure 4: The Initial Positions for Graph Construction. The creation of the graph is a dynamic process which is visualized on our web page. To maintain clarity we chose this particular setting as process initialization, as it gives the most intuition while establishing the final position while not corrupting accuracy of the ideal position approximation.\\n\\nMost promoted articles are the ones with a large number of search-words co-occurrences. Analysis of 20000 publications generated a ranking of the most important publications for each cancer-gene and gene-gene relation. Publications with the best scores are hyperlinked to connections on Figure: 3. A full demonstration is available on our web page.\\n\\n# 6.1.3 Publication context\\n\\nIdentifying the most relevant publication is not the end of the information retrieval process, but the good place to begin. Each scientific paper has directly correlated articles via citations. This enables researchers navigation through the knowledge graph created that way. However, the process of exploration conducted manually provides more problems than solutions:\\n\\n1. Cited publications are not available freely or easily\\n2. There is no easy way of telling which of cited articles is worth reading\\n3. Publication can be cited by hundreds of others. How to get them and distinguish the ones related to our problem?\\n\\nIn order to perform such an extended analysis we used the method from our previous research - the Scientific Paper Advisor [13] tool, which addresses all these issues and solves them by providing an easy-to-use interface (Figure: 5).\\n\\nPresented graph works as an interactive map of currently analyzed publication (dot in the center) and others connected to it via citations. The horizontal axis indicates time passage. The ones on the left are the oldest, the ones on the right are referring to them. Each point represents one article by displaying its connotations, title, authors, and scientific impact indicated by citations (shown as a dot size and directly in a hover). Moreover, each graph node is clickable and redirects to the publications’ content. The user can expand the graph in any preferable direction receiving in that way information about the newest or more fundamental publications in a domain of interest.\\n\\nEach presented publication cites on average 20 others, which gives a massive, exponential growth of nodes in the mentioned graph of knowledge. This means that the width of the consecutive layers would be 1, 20, 400, 8000, and so on. The problem of finding publications referring to a central one goes into even bigger numbers. This is too much information to process by a human, therefore we had to automatically decide what is interesting and what is not.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='403fad13-c865-49f6-aa84-68e29658905f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Vitamin D in thyroid tumorigenesis and development\\n\\nClinckspoor; Verlinden; €. Mathieu\\n\\n#citations: 71\\n\\n# Figure 5: Scientific Publication Citation Network\\n\\nThis directed graph represents the dynamic evolution of scientific knowledge. Each node represents a research article, while the connections indicate citation relationships. The graph provides context by visualizing articles that precede the center node (on the left) and those that build upon its findings (on the right). The size of each node reflects the importance of the publication, measured by the total number of citations. This network serves as a tool for exploring the contextual landscape surrounding a specific scientific publication.\\n\\nThe most commonly used metric identifying the articles’ impact is the citation count. It was used to distinguish what to show and what to hide. It turns out that it can be applied only to long-standing papers. For more recent ones this score gives unpredictable results. In order to overcome this problem, the neural network model was trained to predict citation count.\\n\\nThe combination of the predictive model, automatic publication processing, and previous case-related publication selection finally gave the basis for creating an interactive tool that the researcher can study.\\n\\n# 6.2 Utilization of Large Language Models\\n\\nThe previous approach has shown that the NLP applications can be very case-specific within biomedical research. Manual publication research is time consuming. Here we present a way of utilizing NLP with an advanced LLM-based system to filter out the information of interest from the gathered publications. The best LLMs like GPT-3 [1], GPT-4 [9], Claude, and Mixtral are based on transformer neural network architecture. While these models provide the state-of-the-art capabilities, they can only process a small amount of text at once. This poses significant problems while we want to use such a system for retrieving and presenting knowledge from numerous scientific publications. To overcome this problem augmented generation system was used to retrieve the sufficient amount of text.\\n\\n# 6.2.1 Retrieval-Augmented Generation\\n\\nRetrieval-Augmented Generation (RAG) [7] is a method for integrating external knowledge into the generation process of large language models. It combines the power of pre-trained language models (like GPT-4), with the vast information available in external documents or databases. Here’s how it generally works:\\n\\n1. Retrieval - When the model receives a prompt, it first retrieves relevant documents or data from an external source that could contain useful information for generating the response. This', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='93aa3693-0710-414b-9eb4-23578d29a234', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 6.2.2 Contextual chatbot\\n\\nThe contextual chatbot is a key tool utilized in this research to analyze the context of scientific publications. By leveraging advanced language models like GPT-4, Claude, or Gemini, the chatbot is able to understand and process user queries in the context of relevant publications. This approach allows for more accurate and informed responses by incorporating up-to-date information from external sources.\\n\\nThe chatbot works by taking the user’s message and retrieving the necessary context from a selected source, such as a database of scientific literature. Instead of providing direct answers, the chatbot generates a prompt that combines the user’s query with the retrieved context information. The model then generates a response based on this combined input, resulting in more reliable and precise answers.\\n\\nBy using a contextual chatbot, researchers can access and utilize a vast amount of scientific knowledge to inform their work. The chatbot acts as a bridge between researchers and the wealth of information available, providing accurate and relevant insights from the literature. This technology has the potential to greatly enhance the information retrieval process and support researchers in their quest for knowledge. Answers obtained in such a way are reliable and, based on our users’ judgments, do not contain hallucinations. The final implementation is shown in Figure: 6 with demo available online 3.\\n\\n# 6.2.3 Information Retrieval - Matching Prompts to Contexts\\n\\nThe key component of the retrieval-augmented generation (RAG) system is the information retrieval process, specifically the matching of prompts to relevant contexts. This step is crucial for retrieving the most pertinent information from the database and incorporating it into the generation process.\\n\\nIn this research, the RAG system utilizes a specialized vector database that stores small chunks of text, typically paragraphs, from the knowledge base. Each text is transformed into a numerical representation known as an embedding, which captures its meaning in a numerical format.\\n\\nWhen a prompt is given to the RAG system, it is also transformed into an embedding. The search algorithm then compares the prompt’s embedding to the embeddings of the contexts in the database, using a metric such as cosine similarity to measure the similarity between the prompt and each context.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='38bbcbdf-9d43-4fff-8ad4-a83ee3a34385', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Expression sodium/iodide transporters and thyroid stimulating hormone receptors in thyroid cancer patients and its correlation with iodine nutrition status and pathology\\n\\n# What are the results of the given study?\\n\\nThe results of the study showed that the expression of NIS and TSHR protein was significantly reduced in the thyroid cancer group compared with the normal group:\\n\\n# Tell me more about expression of NIS and TSHR protein\\n\\nWestern blotting results showed that the expression of NIS and TSHR protein was significantly reduced in the thyroid cancer group compared with the normal group: Immunohistochemistry showed the positive expression of NIS and TSHR in thyroid cancer group: The expression of NIS and TSHR mRNA and that in normal group detected by RT-PCR and Western blotting: protein in thyroid cancer patients was significantly lower than.\\n\\n# Figure 6\\n\\nPart of the conversation about publication [4] regarding the correlation of the TSHR gene and thyroid cancer. Conversation taken between bio-medicine expert and semantic question-answering system based on RAG method.\\n\\nThe algorithm retrieves the contexts with the closest embeddings to the prompt, indicating the most relevant information for generating a response. This matching process allows the RAG system to retrieve the most relevant contexts based on the given prompt, ensuring that the generated response incorporates up-to-date and accurate information from the external knowledge base.\\n\\n# 6.2.4 Expanding RAG System with Unlimited Context\\n\\nThe RAG system addresses the limitation of limited source knowledge by extending its capabilities to larger context sizes. While publicly available models from OpenAI can currently handle up to 32k tokens (equivalent to approximately 50 pages of text), our technique connects the model to a database consisting of over 1,000 such 50-page-long publications.\\n\\nWith this expanded context, the RAG system can retrieve and incorporate a significantly larger amount of information for generating responses. The results of querying this robust system are illustrated in Figure 7. The RAG model was tasked with retrieving all relevant information about the correlations of the listed entities, such as the impact of ccRCC on proteins and the influence of genes on thyroid cancer.\\n\\n13', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d684d045-0f0d-4ed5-ab47-3b20f5e5dbcc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Thyroid Carcinoma\\n\\n# CcRCC\\n\\nThyroid carcinoma is associated with genetic alterations such as mutations, gene copy-number gain, and aberrant gene methylation.\\n\\n# Genes\\n\\nPAX8-PPARVI fusion protein has been identified in follicular carcinomas and human thyroid carcinoma.\\n\\n# Proteins\\n\\nProteogenomic measurements uniquely identified protein dysregulation of cellular mechanisms impacted by genomic alterations in CcRCC.\\n\\n# Lipids/fats\\n\\nThe given text does not discuss any relation between Thyroid Carcinoma and lipids/fats. HIFS regulate lipid storage by repressing CPTIA, which forces fatty acids to lipid droplets for storage.\\n\\n# Metabolites\\n\\nThyroid Carcinoma is associated with changes in population-level and institutional-level prescribing habits of radioiodine therapy, changes in total thyroidectomy versus thyroid lobectomy, and changes in circulating cell-free miRNA in plasma. CcRCC is characterized by broad shifts in central carbon metabolism, and antioxidant response, which are associated with increases in glutathione and cysteine/methionine metabolism pathways.\\n\\n# Figure 7: Summarized Answers Table\\n\\nThis table presents the responses generated by our chatbot for various correlations between column and row terms, such as the relationship between proteins and thyroid cancer. Notably, these answers are derived from a significantly larger context, encompassing hundreds of relevant publications.\\n\\nOf particular interest is the pair of lipids and thyroid cancer, where the model showcases an uncommon behavior. Instead of providing speculative answers, the model openly admits that it does not possess knowledge on this specific relationship. This level of transparency and caution is not commonly observed in language models, highlighting the RAG system’s ability to provide more reliable and grounded responses.\\n\\nBy leveraging an extensive database and expanding the context available to the RAG system, researchers can access a significantly larger wealth of knowledge. This advancement allows for more accurate, detailed, and nuanced answers to a wide range of queries, enhancing the information retrieval process in biomedicine.\\n\\n# 6.3 Potential Applications\\n\\nThe research methodology and tools presented in this study have a wide range of potential applications in the field of biomedicine. Some specific scenarios where this research methodology could be applied include:\\n\\n1. Gene Expression Analysis - The methodology demonstrated in this study can be applied to analyze gene expression data in different disease contexts. By identifying key genes and their relationships to specific diseases, researchers can gain a deeper understanding of the underlying mechanisms and potential treatment options.\\n2. Precision Medicine - The integration of advanced NLP techniques and large language models can greatly enhance the field of precision medicine. By leveraging vast amounts of scientific literature, researchers can retrieve and synthesize knowledge to inform personalized treatment plans based on individual patient characteristics.\\n3. Drug Discovery - The information retrieval and analysis techniques showcased in this study can be utilized to expedite the process of drug discovery. By mining scientific literature, researchers can identify potential drug targets, understand the mechanisms of action, and optimize drug design.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68e2f528-2b34-42db-af2a-102fce2aeb0c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 4. Clinical Decision Support\\n\\nThe combination of data analysis and information retrieval can be leveraged to develop clinical decision support systems. These systems can assist healthcare professionals in making informed decisions based on current scientific knowledge and patient-specific data.\\n\\n# 5. Biomarker Identification\\n\\nThe research methodology presented in this study can be applied to identify biomarkers for various diseases. By analyzing gene expression data and mining relevant literature, researchers can uncover potential biomarkers that can aid in early diagnosis, risk assessment, and monitoring of disease progression.\\n\\n# 6. Data Integration\\n\\nBy utilizing NLP techniques, researchers can integrate data from various sources, including electronic health records, genomic databases, and scientific literature. This integration of diverse datasets allows for a more holistic understanding of disease mechanisms and treatment options.\\n\\nAnticipated technological and methodological advancements, such as the continuous improvement of large language models and advancements in data integration techniques, will further enhance the future of NLP in biomedicine. These advancements will enable more accurate and efficient information retrieval, more sophisticated data analysis, and deeper insights into complex biological systems.\\n\\n# 7 Conclusion and Future Work\\n\\n# 7.1 Summary\\n\\nIn this research, we explored the integration of advanced Natural Language Processing (NLP) techniques and large language models to enhance biomedical research. Our study focused on the analysis of gene expression data in the context of cancer predisposition. We replicated and extended a previous study’s findings, identifying the most discriminative genes for cancer prediction. To enrich our analysis and connect it with the scientific literature, we employed text-mining strategies to gather relevant publications and extract key insights. We utilized a combination of traditional NLP approaches and the capabilities of large language models to optimize the information retrieval process and provide accurate and up-to-date knowledge. Our research demonstrates the potential of NLP in improving data analysis, knowledge integration, and decision support in the field of biomedicine. The presented methodology opens opportunities for precise and personalized medicine, drug discovery, biomarker identification, and more. Anticipated future advancements in NLP and data integration will further enhance the capabilities and impact of this research.\\n\\n# 7.2 Future Directions\\n\\nMoving forward, there are several areas of future exploration and improvement for the research conducted in this study. These areas include, but are not limited to:\\n\\n1. Enhancing Information Retrieval - While the methodology presented in this research has demonstrated effective information retrieval and knowledge synthesis, there is still room for improvement. Future work can focus on refining the retrieval algorithms to improve the accuracy and relevance of the retrieved information. This can be achieved through the development of more sophisticated matching techniques such as reranking, and diversifying retrieved paragraphs.\\n2. Expanding the Knowledge Base - The current research utilized a substantial knowledge base of scientific publications. However, the knowledge base can be further expanded to include additional sources of information, such as clinical trial databases, electronic health records, and', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc9b8fe4-c596-419b-94fa-374055dc4985', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Future Directions in NLP for Biomedicine\\n\\n1. Improving Contextual Chatbot Capabilities - While the contextual chatbot used in this research has proven to be a valuable tool for analyzing scientific publications, there is still potential for improvement. Future work can focus on further developing the chatbot’s capabilities, such as improving its understanding of complex queries, generating more detailed responses, and addressing limitations in hallucination or incorrect information.\\n2. Advancements in Large Language Models - The field of large language models is rapidly evolving, with new models and architectures being developed regularly. Future work can explore the application of more advanced models, such as GPT-4, Claude, Gemini, or Mixtral to further enhance the capabilities and performance of the research system. This includes leveraging newer models for both information retrieval and generation tasks, as well as incorporating improvements in areas such as explainability, robustness, and handling of domain-specific knowledge.\\n3. Interdisciplinary Collaboration - The future of NLP in biomedicine lies in strong collaboration between data scientists and biomedical researchers. Future work should focus on fostering interdisciplinary collaboration and knowledge exchange, allowing for the co-development of tools and methodologies that address domain-specific challenges and leverage the expertise of all stakeholders.\\n4. Ethical Considerations - As with any emerging technology, ethical considerations play a crucial role in the future development and deployment of NLP in biomedicine. Future work should address issues such as data privacy, bias in training data, and transparency of model decision-making. It is important to ensure that the benefits of NLP are balanced with ethical considerations to maximize the potential positive impact on biomedical research.\\n\\nAnticipated technological and methodological advancements in NLP, such as the further development of large language models, advancements in data integration techniques, and improvements in explainability and interpretability, will continue to shape the future of NLP in biomedicine. By addressing these future directions, researchers can unlock the full potential of NLP in advancing biomedical knowledge and improving patient care.\\n\\n# References\\n\\n1. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\\n2. Leonardo Collado-Torres, Abhinav Nellore, Kai Kammers, Shannon E Ellis, Margaret A Taub, Kasper D Hansen, Andrew E Jaffe, Ben Langmead, and Jeffrey T Leek. Reproducible rna-seq analysis using recount2. Nature biotechnology, 35(4):319–321, 2017.\\n3. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare, 3(1):1–23, October 2021.\\n4. Li J, Dong JN, Zhao Z, Lv Q, Yun B, Liu JQ, and Cai XY. Expression of sodium/iodide transporters and thyroid stimulating hormone receptors in thyroid cancer patients and its correlation with iodine nutrition status and pathology. 2018.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90636d5b-e923-4e2d-9582-d56493a00cbe', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# References\\n\\n1. Qiao Jin, Won Kim, Qingyu Chen, Donald C. Comeau, Lana Yeganova, W. John Wilbur, and Zhiyong Lu. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval, 2023.\\n2. Alexander Lachmann, Denis Torre, Alexandra B Keenan, Kathleen M Jagodnik, Hoyjin J Lee, Lily Wang, Moshe C Silverstein, and Avi Ma’ayan. Massive mining of publicly available rna-seq data from human and mouse. Nature communications, 9(1):1366, 2018.\\n3. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.\\n4. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6), September 2022.\\n5. OpenAI. Gpt-4 technical report, 2023.\\n6. Jason Priem, Heather Piwowar, and Richard Orr. Openalex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. arXiv preprint arXiv:2205.01833, 2022.\\n7. Aaron M. Smith, Jonathan R. Walsh, John Long, Craig B. Davis, Peter Henstock, Martin R. Hodge, Mateusz Maciejewski, Xinmeng Jasmine Mu, Stephen Ra, Shanrong Zhao, Daniel Ziemek, and Charles K. Fisher. Deep learning of representations for transcriptomics-based phenotype prediction. bioRxiv, 2019.\\n8. Surendrabikram Thapa and Surabhi Adhikari. Chatgpt, bard, and large language models for biomedical research: Opportunities and pitfalls. Annals of Biomedical Engineering, 51(12):2647–2651, 2023.\\n9. Cezary Troska and Antoni D ˛abrowski. Scientific paper advisor a tool for analyzing scientific publications and their links. 2023.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Research\n",
      "\n",
      "| |Samples|Our best model (AUC)|Model from article (AUC)|\n",
      "|---|---|---|---|\n",
      "|COAD_stage|505|68.93%|69.97%|\n",
      "|KIRC_stage|544|77.77%|78.79%|\n",
      "|LIHC_stage|374|67.45%|67.18%|\n",
      "|LUAD_stage|542|65.67%|63.78%|\n",
      "|SKCM_stage|249|64.28%|61.17%|\n",
      "|STAD_stage|416|65.65%|65.19%|\n",
      "|THCA_stage|513|71.08%|70.16%|\n",
      "|UCEC_stage|554|69.62%|68.19%|\n",
      "|LUSC_stage|504|66.04%|66.27%|\n",
      "|BRCA_stage|1134|61.95%|64.70%|\n",
      "|CESC_grade|306|72.63%|72.01%|\n",
      "|KIRC_grade|544|62.66%|62.94%|\n",
      "|LGG_grade|532|77.96%|77.25%|\n",
      "|LIHC_grade|374|68.25%|68.08%|\n",
      "|PAAD_grade|179|66.83%|64.73%|\n",
      "|STAD_grade|416|78.57%|77.83%|\n",
      "|UCEC_grade|554|89.52%|89.56%|\n",
      "|HNSC_grade|504|72.27%|73.56%|\n",
      "|MEAN|MEAN|70.40%|70.08%| |\n",
      "\n",
      "Table 1: The table presents the accuracy scores of the best model from our study compared to the model from the reviewed previous study. It demonstrates that we were able to successfully replicate the findings of the previous study, which focused on identifying the best predictive algorithm. This provides a solid foundation for our further investigations into the interpretability of the model and the identification of the most important genes for prediction.\n",
      "\n",
      "cancer’, ’thyroid carcinoma’ or ’TPO’, ’Thyroid peroxidase’. Subsequently, we employed various natural language processing techniques to extract pertinent information from these texts. Our approach was twofold: one that leveraged traditional text mining methodologies and another that utilized advanced large language models. We advocate this dual-strategy as an effective paradigm for conducting such analytical work in the realm of text mining and information retrieval.\n",
      "\n",
      "# 6.1 Classical NLP approach\n",
      "\n",
      "The Relation Map, shown in Figure: 3, is our way of merging the study results and the intuitions from large amounts of scientific publications. We had to point out the problem we faced. Having multiple thousands of pages of scientific articles regarding the topic we studied, we had to distill the intuition and present it in a digestible way. Our goal was to focus on the ratio of time spent on literature analysis vs the actual knowledge and intuitions gained related to the topic. The Relation Map is the aftermath of such a thought process. It is a joined visualization of key insights from our study shown in the context of scientific literature. Precisely it describes the relationship between two types of cancers and their most discriminating genes. Our analysis showed not only that exactly those eight genes are most influential for prediction but also what is their activation in a certain type of disease - color for indication and thickness for value. After we came to that conclusion, we focused on its scientific connotations.\n",
      "\n",
      "The first metric that we introduced was a Publication Correlation Matrix, which led to understanding which graph nodes should be connected to one another. In order to measure the correlation between two terms e.g. \"ccRCC\" and \"TPO\" we had to filter out only those publications that regard\n"
     ]
    }
   ],
   "source": [
    "print(document[6].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 573cb68c-c65e-424a-922b-365295c89783\n"
     ]
    }
   ],
   "source": [
    "pandas_analysis = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    parsing_instructions=\"\"\"\n",
    "\n",
    "    can you extract all the tables and provide the code for pandas dataframe?\n",
    "\n",
    "    \"\"\"\n",
    "    ).load_data(file_path=\"C:/Users/V. Stasiunaitis/Desktop/RI_BIO_publication.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='6e692d69-58af-4e77-a637-0f09f11f177c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Advancing Biomedical Research Through Natural Language Processing and Large Language Models\\n\\n# A Case Study on Gene Expression Profiling and Knowledge Integration\\n\\nAntoni Czapski\\n\\nRight Information\\n\\n317214@uwr.edu.pl\\n\\nKatarzyna Bloch\\n\\nRight Information\\n\\nkatarzy.bloch@gmail.com\\n\\n# Abstract\\n\\nThe overwhelming volume of complex biomedical data, coupled with the sheer magnitude of scientific literature, makes it exceedingly difficult to process, analyze, and contextualize insights effectively. In this work, we present a pioneering approach to automatic information retrieval using publicly available gene expression profiling data from diverse sources, exemplifying a new paradigm in leveraging advanced natural language processing (NLP) and AI-based data analysis to transform biomedical research. Our methodology not only addresses the complex data analysis issues associated with high-dimensional ’omic’ datasets, but also provides a systematic and insightful way to integrate, interpret, and visualize results in the context of the most relevant scientific literature. We explore this approach in the context of a gene analysis study, showcasing the huge potential of AI-powered techniques to enhance the depth and precision of biomedical research.\\n\\nKey words: biomedical data analysis, natural language processing in genomics, large language models, AI-driven literature mining, gene expression profiling, precision medicine\\n\\n# 1 Introduction\\n\\nThe acquisition of knowledge in the digital era frequently entails a distinct divergence in accessibility between ordinary and scholarly data. Although search engines such as Google are skilled at condensing our questions into easily digestible snippets that promptly provide the information we require, Bing takes it one step beyond by entirely removing the need to explore web pages, offering succinct, tailored answers in a conversational style. One can hardly imagine a more user-friendly experience, resembling a conversation with an expert in the field. Nonetheless, this intuitive searchability is markedly different from navigating scientific databases. Google Scholar, PubMed, and Web of Science seem rudimentary by comparison, lacking the capacity to interpret full-sentence queries, thus leaving users to sift through article titles with brief abstracts. To obtain the required insights, one must frequently navigate through a vast number of intricate and protracted works - an often time-consuming process that doesn’t support the overall goals of most researchers, who strive to ground their work within the broader context of scientific investigation.\\n\\nConsequently, we seek to address this common difficulty encountered by researchers who require a comprehensive perspective for their work. Our study presents a pioneering methodology that combines research findings with a comprehensive understanding of current scientific literature. We outline effective strategies for combining research outputs, automating the curation of relevant studies, and', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b01cdf86-0e13-40f7-bdf9-bcc6cdc82c6b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='transforming this information into easily digestible formats utilizing cutting-edge Large Language Models. This approach creates a new pathway for researchers to interact with scholarly content in a more seamless and efficient manner.\\n\\n# 1.1 Background\\n\\nAs we unravel the complexities of biological systems through scientific endeavors, the intricate web of biomedical data is rapidly expanding. Our pursuit to delve into this ever-growing web of knowledge presents significant challenges, as we strive to extract and integrate information seamlessly. Herein, we explore the current landscape of biomedical information mining, highlighting the ripe opportunities for enhancing the intrepid journey of data to discovery.\\n\\n# Current State of the Biomedical Information Mining\\n\\nBiomedical information mining has remained a monolithic task for researchers, barely evolving in user experience over the last twenty years. Traditionally, researchers approach this Herculean task by conducting keyword searches across databases such as PubMed. They siphon, peruse, and integrate a selection of relevant manuscripts into their work, a labor-intensive ritual that has withstood the test of time. However, beneath the surface, search engine technology has undergone a renaissance, evolving from rudimentary phrase matching to sophisticated neural network-based recommender systems. Nevertheless, the upgrade in technology hasn’t quite permeated the methods by which scientific information is processed and delivered to users; there remains a gulf between advanced backend algorithms and the front-end user experience in scientific data inquiry.\\n\\n# Relevance of Natural Language Processing in Biomedical Research\\n\\nNatural Language Processing (NLP) presents itself as a beacon for refinement in the realm of biomedical research. By simulating the human ability to understand language, NLP provides a powerful set of tools that can pierce through the dense fog of scientific jargon and literature, allowing for more astute aggregation and analysis of research data. It is through NLP that we can pioneer approaches to extract, interpret, and contextualize complex biological data, making them more accessible and informative for researchers.\\n\\n# Technological Advances Enabling NLP in Biomedicine\\n\\nWith advancements in computational power and algorithms, NLP has transitioned from a fledgling technology to a pivotal component in the biomedical informatics arsenal. Breakthroughs in machine learning, particularly in deep learning architectures such as transformers, have greatly enhanced the capabilities of NLP systems. These advancements have enabled NLP to elucidate and process the intricate patterns within large, complex datasets often found in biomedical research, fulfilling tasks from gene annotation to clinical decision support.\\n\\n# Advanced Data Analysis in Genomics\\n\\nThe field of genomics, characterized by its high-dimensional datasets, has advanced significantly, outpacing traditional analytical methodologies. The exponential growth of genomic data necessitates more sophisticated data analysis techniques capable of not just parsing but interpreting the vast and varied biological information. The granularity and depth of genomic studies now require analytical platforms imbued with NLP to mine the depths of relational intricacies among genetic expression profiles, phenotypic variations, and the associated literature.\\n\\nEmbarking on this transformative voyage, we proceed to delineate the methodological advancements of information retrieval, processing, and integration amid the digital milieu. Our endeavor is directed toward reimagining the landscape in which information retrieval is not merely a static function but a dynamic portal to knowledge.\\n\\n# 1. Information Retrieval\\n\\nThe quest begins amid the digital stacks of online repositories, where', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28dc797e-f87c-4678-9c99-8e9b8ae8f1ee', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 1.1 Information Processing\\n\\nbeyond retrieving scientific literature lies the opportunity for similarity searches that align scholarly documents with an investigator’s intellectual pursuits.\\n\\n# 2. Information Processing\\n\\nThe cerebral craftsmanship of a researcher manifests in the ability to decipher, analyze, and synthesize extracted knowledge with the aim of fortifying their investigative repertoire.\\n\\n# 3. Information Integration\\n\\nHere, the empirical evidence garnered interlocks with the edifice of established knowledge, orchestrating a congruent symphony of past insights and novel discoveries.\\n\\nWith this blueprint, we forge ahead to fortify the scaffold upon which data not only informs but transforms, ushering in a new era of artificial intelligence-assisted exploration within the biomedical sphere.\\n\\n# 1.2 Motivation\\n\\nThe current process of background-check before conducting a study relies on retrieving the right articles and extracting domain-specific knowledge from them. The general principle is - the more knowledge the better. However, there is always a limit of time that could be spent on this phase of research. This often leads to underinformed decisions that may significantly impact the value of the study. The optimization of this process enabled by NLP techniques will move the focus of scientists from the manual labor of browsing online repositories with publications to more sophisticated tasks - asking the data right questions and checking whether it can be answered by the current state of domain knowledge represented in collected (automatically) articles.\\n\\nIn the case of our domain of research - analysis of cancer predisposition based on gene expression - as in many others the problem of ’dialog’ between what we do and what the ’Science’ says is undergoing through the whole research. Initially, we conducted a background check. As the research progressed, our focus shifted. We began to explore a new range of interests. This shift necessitated a thorough analysis of the literature about certain genes we deemed most important. Therefore the motivation behind NLP applications extends to merging the new knowledge (acquired during the research) with the current state of the scientific domain.\\n\\nAll the present NLP approaches that we explored presented significant challenges in applications - from search engines discussed previously, to Large Language Models (like GPT family) trained on scientific knowledge. Regarding the second one, we analyze the potential of such methods based on the current state-of-the-art model - BioGPT [8]. The model is based on a relatively small architecture (7 Billion of parameters) that could be run on a small cluster of GPU. It can be utilized on a similar fashion to popular chatbots - users can ask questions and retrieve answers. The benefit of such a model is that out-of-the-box it presents biomedical expertise on the level of a graduate student. While it could be beneficial to discuss the problems with such a model, this approach poses a few unsolvable systematic problems regarding utilization that we described previously. Firstly, when a scientist asks a question crucial for his study, he seeks certainty in the answer, he wants to put the weight of the answer on the shoulders of previous studies, not on the opinion of someone - especially the AI system, that we do not fully understand. Secondly, such models are always outdated. The process of providing new knowledge to the model is achieved only during training, which is done once. To keep the model up-to-date it would be necessary to retrain it on each newly released publication.\\n\\n# 2 Literature Review\\n\\n# Current State of Biomedical Information Retrieval', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='189b27ec-29cb-4ca7-8053-36b34e0e34fb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The contemporary landscape of information retrieval in biomedicine is predominantly concentrated on two pivotal objectives. The first is the development of superior, ready-to-use models—akin to specialized versions of ChatGPT tailored for biomedicine. This involves a competitive effort among researchers to design models capable of accurately answering biomedical queries, as typically encountered in university examinations. Notable instances of such models include MedCPT [5], BioGPT [8], and PubMedBERT [3]. While these chatbots may provide general medical information beneficial to laypersons seeking medical advice, they often fall short in meeting the more nuanced and current knowledge requirements of researchers, thereby limiting their utility in research contexts. The second objective focuses on refining the methods for identifying pertinent knowledge within scientific knowledge repositories. This area has evolved substantially, transitioning from basic word co-occurrence counting techniques to more sophisticated systems that emphasize the semantic comprehension of a paper’s subject matter. However, this evolution in the domain presents a dichotomy for researchers: systems either lack sufficient reliability or are not time-efficient. We propose a synergistic approach, amalgamating the strengths of both paradigms into a unified, end-to-end system that aims to optimize both accuracy and efficiency in biomedical information retrieval.\\n\\n# 3 Research Objectives\\n\\nThis research is driven by two fundamental objectives - technical and visionary. The technical aspect encompasses the execution of a traditional study, adhering to established methodologies and analytical techniques. The visionary aspect, on the other hand, is dedicated to demonstrating and providing insight into the ways in which state-of-the-art technologies in Large Language Models (LLMs) and Natural Language Processing (NLP) can enhance and refine the quality of research. This dual approach not only solidifies the foundational aspects of our study but also propels it towards innovative frontiers in research methodology.\\n\\n# 3.1 Technical\\n\\nWe conducted research to push further the current state of knowledge in the domain of cancer gene predisposition. We took under the lens a previous study [11] by Aaron M. Smith et al. The study was focused on finding the best predictive algorithm that based on gene expression tells whether a patient is healthy or has cancer. To find the algorithm that is truly best, the experiments were taken separately on 50 tasks regarding different cancer types. The need for improvement lies in the poor performance of previous algorithms, the problem was not solved successfully yet. It is the result of two main factors. Authors made an argument that genes rarely act in isolation, so it is reasonable to expect that combinations of genes may be more effective than individual genes for predicting phenotypes. This means that a too-simple predictive model might not catch the complexity of the patterns within the data. On the other hand for each of the 50 tasks we were dealing with about 55 thousand gene expression values per patient. The high amount of data mostly is preferable in ML problems, yet it was not the case here as the sample sizes were significantly smaller - from 100 to 1000 patients. This gives a problem of the ’dimensionality curse’ - each point became so distant from any other that finding patterns and similarities became almost impossible. The goal of our study was to replicate the results of the authors and study further the best algorithm in order to find the most discriminating genes for each tested cancer type.\\n\\n# 3.2 Visionary\\n\\nThe visionary goal refers more to what was discussed before. We wanted to see how much further can be pushed the collaboration between data scientists and domain experts in biomedical research. To be more exact, we wanted to exemplify the pipeline of the following steps:', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5555036a-5bf0-4555-8c7f-4153292efda2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 1. Problem identification\\n\\nretrieving from the conducted research the areas that should be cross-checked with relevant literature\\n\\n# 2. Automatic publication retrieval\\n\\nhow to gather thousands of texts that build the knowledge-base on given topic\\n\\n# 3. Knowledge presentation\\n\\ncreating visualizations that present the intuitions and keypoints about the study results and scientific knowledge\\n\\n# 4. Creation of expert system\\n\\ncreating an interactive system that can be asked about anything within the knowledge base and answers within natural language providing references for each given statement\\n\\nThere are limitations of such techniques as ones discussed previously by Surendrabikram Thapa and Surabhi Adhikar in [12]. However, as we will showcase their concerns about lack of explainability and interoperability, limitations in domain-specific knowledge and many other can be mitigated using clever NLP techniques (e.g. retrieval augmented generation).\\n\\n# 4. Materials and Methods - Main Study\\n\\nThis study employs a comprehensive analysis using publicly accessible, extensive datasets: recount2 [2] and ARCHS4 [6]. These datasets encompass over 50,000 gene expression records from patients with various types of cancer. Our analysis utilizes the data preprocessed by Smith et al. in their research. We engaged 12 distinct sets of refined data, differentiated by the number of genes selected (labeled as ’all’, ’O’, ’OT’) and the applied normalization methods, which include transcript per million (TPM), centered-log-ratio (CLR), Z-score, and Z-ternary. This approach ensures a robust and diverse analysis framework, catering to the complexity and scale of gene expression data in cancer research.\\n\\n# 4.1 Methodology\\n\\nTo advance the analysis of this subject, it was imperative to first replicate the models. For this purpose, we trained all four recommended models across each of the 12 data sets, applying them to 50 distinct tasks. We explored various data embedding methods, including variational autoencoders, stochastic denoising autoencoders, principal component analysis, and scenarios without embedding. To ensure statistical robustness, each experiment was conducted using a 5-fold cross-validation approach. This was particularly crucial given the limited sample size of 100.\\n\\nThe models evaluated in this study included logistic regression, random forest, k-nearest neighbors, and a novel architecture proposed by the original authors. This innovative approach is a semi-supervised representation learning model, capitalizing on the network’s ability to construct representations of a typical healthy patient. This facilitates more effective classification of anomalies, such as cancer patients, by adapting the network to data specific to a particular type of cancer.\\n\\nCumulatively, this comprehensive approach resulted in approximately 50,000 individual experiments, encompassing both training and testing phases. Each experiment was relatively rapid, typically concluding within a minute. While some processes were parallelized to enhance efficiency, the total computation time spanned around three weeks.\\n\\n# 5 Results of Gene Expression Profiling\\n\\nHence, the initial analysis was focused on the identification and evaluation of a model which gives the best predictions. Once logistic regression was identified as the top predictive algorithm the next step', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ae7b9c7b-4c2c-458b-8a04-b324d000d5ef', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Figure 1\\n\\nThe figure presents a performance comparison of all trained models using data from the 5-fold cross-validation process. Each point represents a specific task, plotted on the x-axis, and the corresponding model’s accuracy on the y-axis. The marked points indicate the scores of the best models, which were determined by averaging the performance across all cross-validation trials. It was found that the best model, identified by the orange dot, is logistic regression. Interestingly, this conclusion aligns with the findings of the referenced parent article [11], where the best model’s accuracy is represented by the blue dot.\\n\\nThe analysis was to identify the set of the most important genes. In our case, the top-1 predictive algorithm was logistic regression. It is a shared conclusion between our analysis and one performed in the study [11]. All of the results are shown in Figure 1. The exact comparison is in Table 1.\\n\\nLogistic regression is a simple linear model where the final output passes a sigmoid function. This characteristic enables it to differentiate between categorical rather than numerical data. In principle, the model as it examines the data tries to tune two parameters for each gene - \"activation threshold\" that separates sick from healthy and the certainty level of such a predictor. When the dataset goes through the process of normalization leading to obtaining the normal distribution for each feature, the so-called \"certainty level\" can be used as a determinant of feature importance.\\n\\n# Figure 2\\n\\nThe outcome of the analysis is shown in Figure 2. The visualization of gene importance for cancer prediction gives the opportunity to focus the following research on the ones that matter the most. It leads to new, data-driven questions. Why these particular genes are activated across most patients with thyroid cancer? Were there any other studies analyzing correlations between found genes and examined cancer types? To answer all those and the following questions we performed AI-based literature mining that gave key insights into scientific knowledge. For clarity, this paper will primarily discuss findings related to two specific cancer types: ccRCC and thyroid cancer. However, it should be noted that similar analyses were conducted offline for other cancer types explored in this research.\\n\\n# 6 Results - Text Mining and Information Retrieval\\n\\nFollowing the identification of key genes with significant discriminatory power, our next step was to delve deeper and examine the alignment of our findings with contemporary scientific literature. To accomplish this, we initially gathered a comprehensive collection of scientific publications related to the identified genes and the selected types of cancer. Our primary sources were the Pubmed Open Access Subset1 and articles from the OpenAlex [10] online repository. The selection process involved filtering publications based on the presence of specific key phrases and their synonyms, such as ’thyroid.\\n\\n1 www.ncbi.nlm.nih.gov/pmc/tools/openftlist/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ef92944-8a36-4a13-9a54-8454b4c45090', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Research\\n\\n| |Samples|Our best model (AUC)|Model from article (AUC)|\\n|---|---|---|---|\\n|COAD_stage|505|68.93%|69.97%|\\n|KIRC_stage|544|77.77%|78.79%|\\n|LIHC_stage|374|67.45%|67.18%|\\n|LUAD_stage|542|65.67%|63.78%|\\n|SKCM_stage|249|64.28%|61.17%|\\n|STAD_stage|416|65.65%|65.19%|\\n|THCA_stage|513|71.08%|70.16%|\\n|UCEC_stage|554|69.62%|68.19%|\\n|LUSC_stage|504|66.04%|66.27%|\\n|BRCA_stage|1134|61.95%|64.70%|\\n|CESC_grade|306|72.63%|72.01%|\\n|KIRC_grade|544|62.66%|62.94%|\\n|LGG_grade|532|77.96%|77.25%|\\n|LIHC_grade|374|68.25%|68.08%|\\n|PAAD_grade|179|66.83%|64.73%|\\n|STAD_grade|416|78.57%|77.83%|\\n|UCEC_grade|554|89.52%|89.56%|\\n|HNSC_grade|504|72.27%|73.56%|\\n|MEAN|MEAN|70.40%|70.08%| |\\n\\nTable 1: The table presents the accuracy scores of the best model from our study compared to the model from the reviewed previous study. It demonstrates that we were able to successfully replicate the findings of the previous study, which focused on identifying the best predictive algorithm. This provides a solid foundation for our further investigations into the interpretability of the model and the identification of the most important genes for prediction.\\n\\ncancer’, ’thyroid carcinoma’ or ’TPO’, ’Thyroid peroxidase’. Subsequently, we employed various natural language processing techniques to extract pertinent information from these texts. Our approach was twofold: one that leveraged traditional text mining methodologies and another that utilized advanced large language models. We advocate this dual-strategy as an effective paradigm for conducting such analytical work in the realm of text mining and information retrieval.\\n\\n# 6.1 Classical NLP approach\\n\\nThe Relation Map, shown in Figure: 3, is our way of merging the study results and the intuitions from large amounts of scientific publications. We had to point out the problem we faced. Having multiple thousands of pages of scientific articles regarding the topic we studied, we had to distill the intuition and present it in a digestible way. Our goal was to focus on the ratio of time spent on literature analysis vs the actual knowledge and intuitions gained related to the topic. The Relation Map is the aftermath of such a thought process. It is a joined visualization of key insights from our study shown in the context of scientific literature. Precisely it describes the relationship between two types of cancers and their most discriminating genes. Our analysis showed not only that exactly those eight genes are most influential for prediction but also what is their activation in a certain type of disease - color for indication and thickness for value. After we came to that conclusion, we focused on its scientific connotations.\\n\\nThe first metric that we introduced was a Publication Correlation Matrix, which led to understanding which graph nodes should be connected to one another. In order to measure the correlation between two terms e.g. \"ccRCC\" and \"TPO\" we had to filter out only those publications that regard', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea81a99e-3ce2-4e1c-93bd-e0f14b35b2d6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# HOX7 HOXB-As3\\n\\n# FXYD2 TPO\\n\\n# HOX2\\n\\n# TSHR\\n\\n# HOXB7\\n\\n# IYD\\n\\n# LINCO2036\\n\\n# 1\\n\\n# 1L\\n\\n# Thyroid cancer\\n\\n# ccRCC\\n\\n# Predictiveness of cancer type\\n\\nFigure 2: After identifying the best predictive model, logistic regression, we applied it to distinguish between patients with clear cell renal cell carcinoma (ccRCC) and thyroid cancer. Using normalized gene expression data, we calculated the scaling factors to determine the importance of each gene. The y-axis represents the absolute value of the scaling factor, indicating the gene’s contribution to the classification. The x-axis displays the correlation score, measuring the relationship between gene expressions in patients with either ccRCC (0) or thyroid cancer (1).\\n\\nsuch a connection. We were counting occurrences of \"ccRCC\" (and synonyms) in the \"TPO\" set of articles and respectively then the other way. Then, the logarithm of a joined number was our correlation factor between the two nodes. To highlight the gradient of low- to high-correlated terms we decided to express it as distances between two points.\\n\\n# 6.1.1 Positioning graph nodes\\n\\nWe want to point out the challenge here. As we introduce a new approach it is necessary to create a custom solution. The calculated closeness of two points (both semantic and on our plot) does not lead directly to a final plot. The process of creating a graph requires knowing the positions of points, not the distances between them. In fact, mostly it is impossible to draw points on a 2D plain having their distances. For example distances 3cm, 1cm, 1cm between three points do not satisfy the triangle inequality, therefore a placement presenting the ’closeness’ intuition would put one of the points in superposition. We had to come up with an algorithm for a good-enough approximation of those measures.\\n\\nWe propose an iterative algorithm, that starts with nodes at a given position (Figure: 4) and then slightly moves them to a desired state. As we saw during experiments process converges to a stable position. To validate the result of the algorithm we measured the relative error between the actual distances and desired ones. Mean error differs based on a given problem (i.e. set of genes/cancer types). However, the standard deviation is stable at a level of 5.4% across a hundred different starting positions. This observation convinces us that our approximation is reliable and in fact, visualizes the insights of the data even though the problem is unsolvable.\\n\\nBased on those observations we chose by hand an initial position of points that would be the best for visualization (Figure: 4). Then we presented the whole process of iterative data analysis as frames\\n\\n8', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da79a7d3-6a33-4345-b979-bc8be758a72b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"# Figure 3: Proximity Graph Visualizing Relationships Between Cancers and Their Most Discriminating Genes\\n\\nA visual representation of abstract relations between concepts based on semantic proximity in scientific literature and clinical data. The graph highlights the connections between cancers (ccRCC, thyroid cancer) and their most discriminating genes, indicating gene regulation with color-coded markers (blue for down-regulated, red for up-regulated). The thickness of the lines reflects the magnitude of regulation change from the norm. The distances between nodes represent the semantic correlation, with closer points indicating extensive discussion in the literature. To maintain clarity, only major connections (19 out of 45) are displayed in the graph.\\n\\nin an animation available on our website2.\\n\\n# 6.1.2 Information Retrieval and ranking of publications\\n\\nAs the information retrieval process is the one that actually transcendence our research and goes beyond and within other research studies performed in a similar manner, we want go deeper. The following chapter shows how we retrieve knowledge and rank it from least to most relevant.\\n\\nAutomatic information retrieval consists of two parts - data gathering and data presentation. To collect data, we sent queries to the online repository of the biomedical literature, which returned the list of articles ordered by their certain metric. To organize them according to our interests, i.e. how the article relates to the selected features, we had to define our own ranking.\\n\\nTo produce a ranking, each publication had to be given a relevance score. To do this, we used two metrics. Firstly, we created a vector of occurrences of the 'cancer phrase' in the following parts of the publication: title, abstract, results, conclusions and keywords. Secondly, we created a similar vector with the number of occurrences of the gene. The score was calculated using the following formula:\\n\\n|⃗V |2 + |W|2 + V × WT\\n\\nHence, articles with the highest relevance score where the ones with the highest number of search words co-occurrences. This method takes into account scores of each phrase separately however, the\\n\\nhttps://biomedical.dev6.rightinformation.com/genomic-analysis\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='143e5f03-bee3-41f3-bdf7-5f30d02d85b6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# HOXB7\\n\\n# Legend:\\n\\n|HOXB2|Cancer Type|\\n|---|---|\\n|HOXB-AS3|Gene|\\n|Thyroid|IYD|\\n|ccRCC| |\\n|Hox2| |\\n|SFTA3| |\\n|TSHR| |\\n|TPO| |\\n\\nFigure 4: The Initial Positions for Graph Construction. The creation of the graph is a dynamic process which is visualized on our web page. To maintain clarity we chose this particular setting as process initialization, as it gives the most intuition while establishing the final position while not corrupting accuracy of the ideal position approximation.\\n\\nMost promoted articles are the ones with a large number of search-words co-occurrences. Analysis of 20000 publications generated a ranking of the most important publications for each cancer-gene and gene-gene relation. Publications with the best scores are hyperlinked to connections on Figure: 3. A full demonstration is available on our web page.\\n\\n# 6.1.3 Publication context\\n\\nIdentifying the most relevant publication is not the end of the information retrieval process, but the good place to begin. Each scientific paper has directly correlated articles via citations. This enables researchers navigation through the knowledge graph created that way. However, the process of exploration conducted manually provides more problems than solutions:\\n\\n1. Cited publications are not available freely or easily\\n2. There is no easy way of telling which of cited articles is worth reading\\n3. Publication can be cited by hundreds of others. How to get them and distinguish the ones related to our problem?\\n\\nIn order to perform such an extended analysis we used the method from our previous research - the Scientific Paper Advisor [13] tool, which addresses all these issues and solves them by providing an easy-to-use interface (Figure: 5).\\n\\nPresented graph works as an interactive map of currently analyzed publication (dot in the center) and others connected to it via citations. The horizontal axis indicates time passage. The ones on the left are the oldest, the ones on the right are referring to them. Each point represents one article by displaying its connotations, title, authors, and scientific impact indicated by citations (shown as a dot size and directly in a hover). Moreover, each graph node is clickable and redirects to the publications’ content. The user can expand the graph in any preferable direction receiving in that way information about the newest or more fundamental publications in a domain of interest.\\n\\nEach presented publication cites on average 20 others, which gives a massive, exponential growth of nodes in the mentioned graph of knowledge. This means that the width of the consecutive layers would be 1, 20, 400, 8000, and so on. The problem of finding publications referring to a central one goes into even bigger numbers. This is too much information to process by a human, therefore we had to automatically decide what is interesting and what is not.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='30d2f58c-fb42-4cbf-9cf5-f5c0c687e206', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Vitamin D in thyroid tumorigenesis and development\\n\\nClinckspoor; Verlinden; €. Mathieu\\n\\n#citations: 71\\n\\n# Figure 5: Scientific Publication Citation Network\\n\\nThis directed graph represents the dynamic evolution of scientific knowledge. Each node represents a research article, while the connections indicate citation relationships. The graph provides context by visualizing articles that precede the center node (on the left) and those that build upon its findings (on the right). The size of each node reflects the importance of the publication, measured by the total number of citations. This network serves as a tool for exploring the contextual landscape surrounding a specific scientific publication.\\n\\nThe most commonly used metric identifying the articles’ impact is the citation count. It was used to distinguish what to show and what to hide. It turns out that it can be applied only to long-standing papers. For more recent ones this score gives unpredictable results. In order to overcome this problem, the neural network model was trained to predict citation count.\\n\\nThe combination of the predictive model, automatic publication processing, and previous case-related publication selection finally gave the basis for creating an interactive tool that the researcher can study.\\n\\n# 6.2 Utilization of Large Language Models\\n\\nThe previous approach has shown that the NLP applications can be very case-specific within biomedical research. Manual publication research is time consuming. Here we present a way of utilizing NLP with an advanced LLM-based system to filter out the information of interest from the gathered publications. The best LLMs like GPT-3 [1], GPT-4 [9], Claude, and Mixtral are based on transformer neural network architecture. While these models provide the state-of-the-art capabilities, they can only process a small amount of text at once. This poses significant problems while we want to use such a system for retrieving and presenting knowledge from numerous scientific publications. To overcome this problem augmented generation system was used to retrieve the sufficient amount of text.\\n\\n# 6.2.1 Retrieval-Augmented Generation\\n\\nRetrieval-Augmented Generation (RAG) [7] is a method for integrating external knowledge into the generation process of large language models. It combines the power of pre-trained language models (like GPT-4), with the vast information available in external documents or databases. Here’s how it generally works:\\n\\n1. Retrieval - When the model receives a prompt, it first retrieves relevant documents or data from an external source that could contain useful information for generating the response. This', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3cc00262-fd2d-4f79-89a8-33dc2215f83d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 6.2.2 Contextual chatbot\\n\\nThe contextual chatbot is a key tool utilized in this research to analyze the context of scientific publications. By leveraging advanced language models like GPT-4, Claude, or Gemini, the chatbot is able to understand and process user queries in the context of relevant publications. This approach allows for more accurate and informed responses by incorporating up-to-date information from external sources.\\n\\nThe chatbot works by taking the user’s message and retrieving the necessary context from a selected source, such as a database of scientific literature. Instead of providing direct answers, the chatbot generates a prompt that combines the user’s query with the retrieved context information. The model then generates a response based on this combined input, resulting in more reliable and precise answers.\\n\\nBy using a contextual chatbot, researchers can access and utilize a vast amount of scientific knowledge to inform their work. The chatbot acts as a bridge between researchers and the wealth of information available, providing accurate and relevant insights from the literature. This technology has the potential to greatly enhance the information retrieval process and support researchers in their quest for knowledge. Answers obtained in such a way are reliable and, based on our users’ judgments, do not contain hallucinations. The final implementation is shown in Figure: 6 with demo available online 3.\\n\\n# 6.2.3 Information Retrieval - Matching Prompts to Contexts\\n\\nThe key component of the retrieval-augmented generation (RAG) system is the information retrieval process, specifically the matching of prompts to relevant contexts. This step is crucial for retrieving the most pertinent information from the database and incorporating it into the generation process.\\n\\nIn this research, the RAG system utilizes a specialized vector database that stores small chunks of text, typically paragraphs, from the knowledge base. Each text is transformed into a numerical representation known as an embedding, which captures its meaning in a numerical format.\\n\\nWhen a prompt is given to the RAG system, it is also transformed into an embedding. The search algorithm then compares the prompt’s embedding to the embeddings of the contexts in the database, using a metric such as cosine similarity to measure the similarity between the prompt and each context.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='07e66f1c-eb6f-4e10-b5f9-19bc0302ede1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Expression sodium/iodide transporters and thyroid stimulating hormone receptors in thyroid cancer patients and its correlation with iodine nutrition status and pathology\\n\\n# What are the results of the given study?\\n\\nThe results of the study showed that the expression of NIS and TSHR protein was significantly reduced in the thyroid cancer group compared with the normal group:\\n\\n# Tell me more about expression of NIS and TSHR protein\\n\\nWestern blotting results showed that the expression of NIS and TSHR protein was significantly reduced in the thyroid cancer group compared with the normal group: Immunohistochemistry showed the positive expression of NIS and TSHR in thyroid cancer group: The expression of NIS and TSHR mRNA and that in normal group detected by RT-PCR and Western blotting: protein in thyroid cancer patients was significantly lower than.\\n\\n# Figure 6\\n\\nPart of the conversation about publication [4] regarding the correlation of the TSHR gene and thyroid cancer. Conversation taken between bio-medicine expert and semantic question-answering system based on RAG method.\\n\\nThe algorithm retrieves the contexts with the closest embeddings to the prompt, indicating the most relevant information for generating a response. This matching process allows the RAG system to retrieve the most relevant contexts based on the given prompt, ensuring that the generated response incorporates up-to-date and accurate information from the external knowledge base.\\n\\n# 6.2.4 Expanding RAG System with Unlimited Context\\n\\nThe RAG system addresses the limitation of limited source knowledge by extending its capabilities to larger context sizes. While publicly available models from OpenAI can currently handle up to 32k tokens (equivalent to approximately 50 pages of text), our technique connects the model to a database consisting of over 1,000 such 50-page-long publications.\\n\\nWith this expanded context, the RAG system can retrieve and incorporate a significantly larger amount of information for generating responses. The results of querying this robust system are illustrated in Figure 7. The RAG model was tasked with retrieving all relevant information about the correlations of the listed entities, such as the impact of ccRCC on proteins and the influence of genes on thyroid cancer.\\n\\n13', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1e285fbb-4206-4247-a607-76e1d7bf2829', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Thyroid Carcinoma\\n\\n# CcRCC\\n\\nThyroid carcinoma is associated with genetic alterations such as mutations, gene copy-number gain, and aberrant gene methylation.\\n\\n# Genes\\n\\nPAX8-PPARVI fusion protein has been identified in follicular carcinomas and human thyroid carcinoma.\\n\\n# Proteins\\n\\nProteogenomic measurements uniquely identified protein dysregulation of cellular mechanisms impacted by genomic alterations in CcRCC.\\n\\n# Lipids/fats\\n\\nThe given text does not discuss any relation between Thyroid Carcinoma and lipids/fats. HIFS regulate lipid storage by repressing CPTIA, which forces fatty acids to lipid droplets for storage.\\n\\n# Metabolites\\n\\nThyroid Carcinoma is associated with changes in population-level and institutional-level prescribing habits of radioiodine therapy, changes in total thyroidectomy versus thyroid lobectomy, and changes in circulating cell-free miRNA in plasma. CcRCC is characterized by broad shifts in central carbon metabolism, and antioxidant response, which are associated with increases in glutathione and cysteine/methionine metabolism pathways.\\n\\n# Figure 7: Summarized Answers Table\\n\\nThis table presents the responses generated by our chatbot for various correlations between column and row terms, such as the relationship between proteins and thyroid cancer. Notably, these answers are derived from a significantly larger context, encompassing hundreds of relevant publications.\\n\\nOf particular interest is the pair of lipids and thyroid cancer, where the model showcases an uncommon behavior. Instead of providing speculative answers, the model openly admits that it does not possess knowledge on this specific relationship. This level of transparency and caution is not commonly observed in language models, highlighting the RAG system’s ability to provide more reliable and grounded responses.\\n\\nBy leveraging an extensive database and expanding the context available to the RAG system, researchers can access a significantly larger wealth of knowledge. This advancement allows for more accurate, detailed, and nuanced answers to a wide range of queries, enhancing the information retrieval process in biomedicine.\\n\\n# 6.3 Potential Applications\\n\\nThe research methodology and tools presented in this study have a wide range of potential applications in the field of biomedicine. Some specific scenarios where this research methodology could be applied include:\\n\\n1. Gene Expression Analysis - The methodology demonstrated in this study can be applied to analyze gene expression data in different disease contexts. By identifying key genes and their relationships to specific diseases, researchers can gain a deeper understanding of the underlying mechanisms and potential treatment options.\\n2. Precision Medicine - The integration of advanced NLP techniques and large language models can greatly enhance the field of precision medicine. By leveraging vast amounts of scientific literature, researchers can retrieve and synthesize knowledge to inform personalized treatment plans based on individual patient characteristics.\\n3. Drug Discovery - The information retrieval and analysis techniques showcased in this study can be utilized to expedite the process of drug discovery. By mining scientific literature, researchers can identify potential drug targets, understand the mechanisms of action, and optimize drug design.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='63826cce-60d5-4579-9403-c79eecfacbdb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 4. Clinical Decision Support\\n\\nThe combination of data analysis and information retrieval can be leveraged to develop clinical decision support systems. These systems can assist healthcare professionals in making informed decisions based on current scientific knowledge and patient-specific data.\\n\\n# 5. Biomarker Identification\\n\\nThe research methodology presented in this study can be applied to identify biomarkers for various diseases. By analyzing gene expression data and mining relevant literature, researchers can uncover potential biomarkers that can aid in early diagnosis, risk assessment, and monitoring of disease progression.\\n\\n# 6. Data Integration\\n\\nBy utilizing NLP techniques, researchers can integrate data from various sources, including electronic health records, genomic databases, and scientific literature. This integration of diverse datasets allows for a more holistic understanding of disease mechanisms and treatment options.\\n\\nAnticipated technological and methodological advancements, such as the continuous improvement of large language models and advancements in data integration techniques, will further enhance the future of NLP in biomedicine. These advancements will enable more accurate and efficient information retrieval, more sophisticated data analysis, and deeper insights into complex biological systems.\\n\\n# 7 Conclusion and Future Work\\n\\n# 7.1 Summary\\n\\nIn this research, we explored the integration of advanced Natural Language Processing (NLP) techniques and large language models to enhance biomedical research. Our study focused on the analysis of gene expression data in the context of cancer predisposition. We replicated and extended a previous study’s findings, identifying the most discriminative genes for cancer prediction. To enrich our analysis and connect it with the scientific literature, we employed text-mining strategies to gather relevant publications and extract key insights. We utilized a combination of traditional NLP approaches and the capabilities of large language models to optimize the information retrieval process and provide accurate and up-to-date knowledge. Our research demonstrates the potential of NLP in improving data analysis, knowledge integration, and decision support in the field of biomedicine. The presented methodology opens opportunities for precise and personalized medicine, drug discovery, biomarker identification, and more. Anticipated future advancements in NLP and data integration will further enhance the capabilities and impact of this research.\\n\\n# 7.2 Future Directions\\n\\nMoving forward, there are several areas of future exploration and improvement for the research conducted in this study. These areas include, but are not limited to:\\n\\n1. Enhancing Information Retrieval - While the methodology presented in this research has demonstrated effective information retrieval and knowledge synthesis, there is still room for improvement. Future work can focus on refining the retrieval algorithms to improve the accuracy and relevance of the retrieved information. This can be achieved through the development of more sophisticated matching techniques such as reranking, and diversifying retrieved paragraphs.\\n2. Expanding the Knowledge Base - The current research utilized a substantial knowledge base of scientific publications. However, the knowledge base can be further expanded to include additional sources of information, such as clinical trial databases, electronic health records, and', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='955589ec-0b45-4378-869f-d76466713555', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Future Directions in NLP for Biomedicine\\n\\n1. Improving Contextual Chatbot Capabilities - While the contextual chatbot used in this research has proven to be a valuable tool for analyzing scientific publications, there is still potential for improvement. Future work can focus on further developing the chatbot’s capabilities, such as improving its understanding of complex queries, generating more detailed responses, and addressing limitations in hallucination or incorrect information.\\n2. Advancements in Large Language Models - The field of large language models is rapidly evolving, with new models and architectures being developed regularly. Future work can explore the application of more advanced models, such as GPT-4, Claude, Gemini, or Mixtral to further enhance the capabilities and performance of the research system. This includes leveraging newer models for both information retrieval and generation tasks, as well as incorporating improvements in areas such as explainability, robustness, and handling of domain-specific knowledge.\\n3. Interdisciplinary Collaboration - The future of NLP in biomedicine lies in strong collaboration between data scientists and biomedical researchers. Future work should focus on fostering interdisciplinary collaboration and knowledge exchange, allowing for the co-development of tools and methodologies that address domain-specific challenges and leverage the expertise of all stakeholders.\\n4. Ethical Considerations - As with any emerging technology, ethical considerations play a crucial role in the future development and deployment of NLP in biomedicine. Future work should address issues such as data privacy, bias in training data, and transparency of model decision-making. It is important to ensure that the benefits of NLP are balanced with ethical considerations to maximize the potential positive impact on biomedical research.\\n\\nAnticipated technological and methodological advancements in NLP, such as the further development of large language models, advancements in data integration techniques, and improvements in explainability and interpretability, will continue to shape the future of NLP in biomedicine. By addressing these future directions, researchers can unlock the full potential of NLP in advancing biomedical knowledge and improving patient care.\\n\\n# References\\n\\n1. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\\n2. Leonardo Collado-Torres, Abhinav Nellore, Kai Kammers, Shannon E Ellis, Margaret A Taub, Kasper D Hansen, Andrew E Jaffe, Ben Langmead, and Jeffrey T Leek. Reproducible rna-seq analysis using recount2. Nature biotechnology, 35(4):319–321, 2017.\\n3. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare, 3(1):1–23, October 2021.\\n4. Li J, Dong JN, Zhao Z, Lv Q, Yun B, Liu JQ, and Cai XY. Expression of sodium/iodide transporters and thyroid stimulating hormone receptors in thyroid cancer patients and its correlation with iodine nutrition status and pathology. 2018.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='87d6de02-8b44-4f2e-8caa-9d88c5880a23', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# References\\n\\n1. Qiao Jin, Won Kim, Qingyu Chen, Donald C. Comeau, Lana Yeganova, W. John Wilbur, and Zhiyong Lu. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval, 2023.\\n2. Alexander Lachmann, Denis Torre, Alexandra B Keenan, Kathleen M Jagodnik, Hoyjin J Lee, Lily Wang, Moshe C Silverstein, and Avi Ma’ayan. Massive mining of publicly available rna-seq data from human and mouse. Nature communications, 9(1):1366, 2018.\\n3. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.\\n4. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6), September 2022.\\n5. OpenAI. Gpt-4 technical report, 2023.\\n6. Jason Priem, Heather Piwowar, and Richard Orr. Openalex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. arXiv preprint arXiv:2205.01833, 2022.\\n7. Aaron M. Smith, Jonathan R. Walsh, John Long, Craig B. Davis, Peter Henstock, Martin R. Hodge, Mateusz Maciejewski, Xinmeng Jasmine Mu, Stephen Ra, Shanrong Zhao, Daniel Ziemek, and Charles K. Fisher. Deep learning of representations for transcriptomics-based phenotype prediction. bioRxiv, 2019.\\n8. Surendrabikram Thapa and Surabhi Adhikari. Chatgpt, bard, and large language models for biomedical research: Opportunities and pitfalls. Annals of Biomedical Engineering, 51(12):2647–2651, 2023.\\n9. Cezary Troska and Antoni D ˛abrowski. Scientific paper advisor a tool for analyzing scientific publications and their links. 2023.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
